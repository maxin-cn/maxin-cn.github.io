<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Blog | Xin Ma</title>
    <link>https://maxin-cn.github.io/post/</link>
      <atom:link href="https://maxin-cn.github.io/post/index.xml" rel="self" type="application/rss+xml" />
    <description>Blog</description>
    <generator>Hugo Blox Builder (https://hugoblox.com)</generator><language>en-us</language><lastBuildDate>Sun, 08 Feb 2026 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://maxin-cn.github.io/media/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_3.png</url>
      <title>Blog</title>
      <link>https://maxin-cn.github.io/post/</link>
    </image>
    
    <item>
      <title>üéâ Our paper MiraMo was accpeted by IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
      <link>https://maxin-cn.github.io/post/miramo-tpami/</link>
      <pubDate>Sun, 08 Feb 2026 00:00:00 +0000</pubDate>
      <guid>https://maxin-cn.github.io/post/miramo-tpami/</guid>
      <description>&lt;p&gt;one paper &lt;a href=&#34;../../publication/journal-article/miramo_tpami_2026/&#34;&gt;MiraMo&lt;/a&gt; was accpeted by IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)&lt;/p&gt;
&lt;h2 id=&#34;abstract&#34;&gt;Abstract&lt;/h2&gt;
&lt;p&gt;Image animation has seen significant progress, driven by the powerful generative capabilities of diffusion models.
However, maintaining appearance consistency with static input images and mitigating abrupt motion transitions in generated animations remain persistent challenges. While text-to-video (T2V) generation has demonstrated impressive performance with diffusion transformer models, the image animation field still largely relies on U-Net-based diffusion models, which lag behind the latest T2V approaches. Moreover, the quadratic complexity of vanilla self-attention mechanisms in Transformers imposes heavy computational demands, making image animation particularly resource-intensive.
To address these issues, we propose MiraMo, a framework designed to enhance efficiency, appearance consistency, and motion smoothness in image animation.
Specifically, MiraMo introduces three key elements: (1) A foundational text-to-video architecture replacing vanilla self-attention with efficient linear attention to reduce computational overhead while preserving generation quality; (2) A novel motion residual learning paradigm that focuses on modeling motion dynamics rather than directly predicting frames, improving temporal consistency; and (3) A DCT-based noise refinement strategy during inference to suppress sudden motion artifacts, complemented by a dynamics control module to balance motion smoothness and expressiveness.
Extensive experiments against state-of-the-art methods validate the superiority of MiraMo in generating consistent, smooth, and controllable animations with accelerated inference speed. Additionally, we demonstrate the versatility of MiraMo through applications in motion transfer and video editing tasks. The project page is available at: &lt;a href=&#34;https://maxin-cn.github.io/miramo_project&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://maxin-cn.github.io/miramo_project&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>‚¨ÜÔ∏è one paper Miramo exploring image animation using linear attention was released on arXiv</title>
      <link>https://maxin-cn.github.io/post/miramo-paper/</link>
      <pubDate>Sun, 10 Aug 2025 00:00:00 +0000</pubDate>
      <guid>https://maxin-cn.github.io/post/miramo-paper/</guid>
      <description>&lt;!-- Our paper [MiraMo](../../publication/preprint/miramo/) was released on [arxiv](https://arxiv.org/abs/2508.07246). --&gt;
&lt;p&gt;We have released &lt;a href=&#34;../../publication/preprint/miramo/&#34;&gt;MiraMo&lt;/a&gt; on arXiv. It is among the first papers to explore linear attention for video generation.&lt;/p&gt;
&lt;h2 id=&#34;setup&#34;&gt;Setup&lt;/h2&gt;
&lt;p&gt;Download and set up the repo:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;
&lt;pre class=&#34;chroma&#34;&gt;
&lt;code&gt;
git clone https://github.com/maxin-cn/MiraMo
cd MiraMo
conda env create -f environment.yml
conda activate miramo
&lt;/code&gt;
&lt;/pre&gt;
&lt;/div&gt;
&lt;h2 id=&#34;animation&#34;&gt;Animation&lt;/h2&gt;
&lt;p&gt;You can sample from our pre-trained MiraMo models. Weights for our pre-trained MiraMo model can be found &lt;a href=&#34;https://huggingface.co/maxin-cn/MiraMo/tree/main&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;. The script has various arguments for adjusting sampling steps, changing the classifier-free guidance scale, etc:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;
&lt;pre class=&#34;chroma&#34;&gt;
&lt;code&gt;
bash pipelines/animation.sh
&lt;/code&gt;
&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;Related model weights will be downloaded automatically, and the following results can be found &lt;a href=&#34;https://github.com/maxin-cn/MiraMo/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>üéâ one paper for the cartoon animation was accpeted by Pacific Conference on Computer Graphics and Applications (PG) 2025</title>
      <link>https://maxin-cn.github.io/post/cartoon-pg2025/</link>
      <pubDate>Sat, 09 Aug 2025 00:00:00 +0000</pubDate>
      <guid>https://maxin-cn.github.io/post/cartoon-pg2025/</guid>
      <description>&lt;p&gt;Our paper, titled &lt;a href=&#34;../../publication/conference-paper/pg_2025/&#34;&gt;‚ÄúTrajectory-guided Anime Video Synthesis via Effective Motion Learning‚Äù&lt;/a&gt;, which focuses on cartoon and anime motion production, has been accepted to Pacific Graphics (PG).&lt;/p&gt;
&lt;h2 id=&#34;abstract&#34;&gt;Abstract&lt;/h2&gt;
&lt;p&gt;Cartoon and anime motion production is traditionally labor-intensive, requiring detailed animatics and extensive in-betweening from keyframes. To streamline this process, we propose a novel framework that synthesizes motion directly from a single colored keyframe, guided by user-provided trajectories. Addressing the limitations of prior methods, which struggle with anime due to reliance on optical flow estimators and models trained on natural videos, we introduce an efficient motion representation specifically adapted for anime, leveraging CoTracker to capture sparse frame-to-frame tracking effectively. To achieve our objective, we design a two-stage learning mechanism: the first stage predicts sparse motion from input frames and trajectories, generating a motion preview sequence via explicit warping; the second stage refines these previews into high-quality anime frames by fine-tuning ToonCrafter, an anime-specific video diffusion model. We train our framework on a novel animation video dataset comprising more than 500,000 clips. Experimental results demonstrate significant improvements in animating still frames, achieving better alignment with user-provided trajectories and more natural motion patterns while preserving anime stylization and visual quality. Our method also supports versatile applications, including motion manga generation and 2D vector graphic animations.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>üë©üèº‚Äçüè´ Graduating in late 2025 to early 2026 and currently seeking full-time job opportunities. Please contact me if you are recruiting</title>
      <link>https://maxin-cn.github.io/post/looking-job/</link>
      <pubDate>Thu, 10 Jul 2025 00:00:00 +0000</pubDate>
      <guid>https://maxin-cn.github.io/post/looking-job/</guid>
      <description>&lt;p&gt;I am expected to graduate between late 2025 and early 2026, and I am currently seeking full-time opportunities. If you are interested, please contact me. Many thanks~&lt;/p&gt;
&lt;h2 id=&#34;my-research-interest&#34;&gt;My Research Interest&lt;/h2&gt;
&lt;p&gt;I am focus on video and image generation, multimodal models, low-level vision, and face recognition, among others.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>üéâ I was recognized as the outstanding reviewer for CVPR 2025 and won the CVPR 2025 Travel Support Award</title>
      <link>https://maxin-cn.github.io/post/outstanding-reviewer-cvpr2025/</link>
      <pubDate>Fri, 13 Jun 2025 00:00:00 +0000</pubDate>
      <guid>https://maxin-cn.github.io/post/outstanding-reviewer-cvpr2025/</guid>
      <description>&lt;p&gt;I am honored to have been selected as an Outstanding Reviewer by the CVPR 2025 committee. Earlier, I was also awarded the CVPR 2025 Travel Support Award.&lt;/p&gt;
&lt;h2 id=&#34;details-of-the-outstanding-reviewers-for-cvpr-2025&#34;&gt;Details of the Outstanding Reviewers for CVPR 2025&lt;/h2&gt;
&lt;p&gt;This recognition reflects not only the quality and depth of the reviews but also the commitment to fairness, clarity, and timely feedback throughout the review process. A total of 12,593 reviewers submitted reviews for CVPR 2025. A total of 711 outstanding reviewers are chosen from reviewers who have submitted at least 3 reviews and are highly rated by area chairs. Total list of outstanding reviewers is available &lt;a href=&#34;https://cvpr.thecvf.com/Conferences/2025/ProgramCommittee&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;details-of-the-travel-support-award-for-cvpr-2025&#34;&gt;Details of the Travel Support Award for CVPR 2025&lt;/h2&gt;
&lt;p&gt;The CVPR 2025 Travel Support Award provides partial coverage for travel expenses and waives the registration fee.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>üíûÔ∏è Always pursuse research collaborations. Feel free to contact me if you are interested</title>
      <link>https://maxin-cn.github.io/post/research-collaboration/</link>
      <pubDate>Mon, 02 Jun 2025 00:00:00 +0000</pubDate>
      <guid>https://maxin-cn.github.io/post/research-collaboration/</guid>
      <description>&lt;p&gt;I am always pursuse research collaborations. Feel free to contact me if you are interested.&lt;/p&gt;
&lt;h2 id=&#34;my-research-interest&#34;&gt;My Research Interest&lt;/h2&gt;
&lt;p&gt;I am focus on video and image generation, multimodal models, low-level vision, and face recognition, among others.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>‚¨ÜÔ∏è one paper OmniPainter for the sylized T2I generation was released on arXiv</title>
      <link>https://maxin-cn.github.io/post/omnipainter-code/</link>
      <pubDate>Tue, 27 May 2025 00:00:00 +0000</pubDate>
      <guid>https://maxin-cn.github.io/post/omnipainter-code/</guid>
      <description>&lt;p&gt;Our paper &lt;a href=&#34;../../publication/preprint/omnipainter/&#34;&gt;OmniPainter&lt;/a&gt; for the sylized text-to-image generation was released on &lt;a href=&#34;https://arxiv.org/abs/2505.19063&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;arxiv&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;setup&#34;&gt;Setup&lt;/h2&gt;
&lt;p&gt;Download and set up the repo:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;
&lt;pre class=&#34;chroma&#34;&gt;
&lt;code&gt;
git clone https://github.com/maxin-cn/OmniPainter
cd OmniPainter
conda env create -f environment.yml
conda activate omnipainter
&lt;/code&gt;
&lt;/pre&gt;
&lt;/div&gt;
&lt;h2 id=&#34;stylized-image-generation&#34;&gt;Stylized image generation&lt;/h2&gt;
&lt;p&gt;You can sample high-quality images that match both the given prompt and the style reference image within just 4 to 6 timesteps, without requiring any inversion. The script has various arguments for adjusting sampling steps, changing the classifier-free guidance scale, etc:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;
&lt;pre class=&#34;chroma&#34;&gt;
&lt;code&gt;
bash run.sh
&lt;/code&gt;
&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;Related model weights will be downloaded automatically and following results can be obtained &lt;a href=&#34;https://github.com/maxin-cn/OmniPainter/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>üéâ one paper Latte was accpeted by Transactions on Machine Learning Research (TMLR)</title>
      <link>https://maxin-cn.github.io/post/latte-tmlr/</link>
      <pubDate>Sun, 23 Mar 2025 00:00:00 +0000</pubDate>
      <guid>https://maxin-cn.github.io/post/latte-tmlr/</guid>
      <description>&lt;p&gt;Our paper &lt;a href=&#34;../../publication/journal-article/tmlr_latte_2025/&#34;&gt;Latte&lt;/a&gt; was accpeted by Transactions on Machine Learning Research (TMLR)&lt;/p&gt;
&lt;h2 id=&#34;abstract&#34;&gt;Abstract&lt;/h2&gt;
&lt;p&gt;We propose Latte, a novel Latent Diffusion Transformer for video generation. Latte first extracts spatio-temporal tokens from input videos and then adopts a series of Transformer blocks to model video distribution in the latent space. In order to model a substantial number of tokens extracted from videos, four efficient variants are introduced from the perspective of decomposing the spatial and temporal dimensions of input videos. To improve the quality of generated videos, we determine the best practices of Latte through rigorous experimental analysis, including video clip patch embedding, model variants, timestep-class information injection, temporal positional embedding, and learning strategies. Our comprehensive evaluation demonstrates that Latte achieves state-of-the-art performance across four standard video generation datasets, i.e., FaceForensics, SkyTimelapse, UCF101, and Taichi-HD. In addition, we extend Latte to the text-to-video generation (T2V) task, where Latte achieves results that are competitive with recent T2V models. We strongly believe that Latte provides valuable insights for future research on incorporating Transformers into diffusion models for video generation. Project page: &lt;a href=&#34;https://maxin-cn.github.io/latte_project&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://maxin-cn.github.io/latte_project&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;inference&#34;&gt;Inference&lt;/h2&gt;
&lt;p&gt;Latte-1 is now integrated into diffusers.&lt;/p&gt;
&lt;p&gt;You can easily run Latte using the following code. We also support inference with 4/8-bit quantization, which can reduce GPU memory from 17 GB to 9 GB.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;
&lt;pre class=&#34;chroma&#34;&gt;
&lt;code&gt;
#Please update the version of diffusers at leaset to 0.30.0
from diffusers import LattePipeline
from diffusers.models import AutoencoderKLTemporalDecoder
from torchvision.utils import save_image
import torch
import imageio
&lt;p&gt;torch.manual_seed(0)&lt;/p&gt;
&lt;p&gt;device = &amp;ldquo;cuda&amp;rdquo; if torch.cuda.is_available() else &amp;ldquo;cpu&amp;rdquo;
video_length = 16 # 1 (text-to-image) or 16 (text-to-video)
pipe = LattePipeline.from_pretrained(&amp;ldquo;maxin-cn/Latte-1&amp;rdquo;, torch_dtype=torch.float16).to(device)&lt;/p&gt;
&lt;p&gt;#Using temporal decoder of VAE
vae = AutoencoderKLTemporalDecoder.from_pretrained(&amp;ldquo;maxin-cn/Latte-1&amp;rdquo;, subfolder=&amp;ldquo;vae_temporal_decoder&amp;rdquo;, torch_dtype=torch.float16).to(device)
pipe.vae = vae&lt;/p&gt;
&lt;p&gt;prompt = &amp;ldquo;a cat wearing sunglasses and working as a lifeguard at pool.&amp;rdquo;
videos = pipe(prompt, video_length=video_length, output_type=&amp;lsquo;pt&amp;rsquo;).frames.cpu()
&lt;/code&gt;
&lt;/pre&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>üéâ one paper Cinemo was accpeted by IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) 2025</title>
      <link>https://maxin-cn.github.io/post/cinemo-cvpr/</link>
      <pubDate>Thu, 27 Feb 2025 00:00:00 +0000</pubDate>
      <guid>https://maxin-cn.github.io/post/cinemo-cvpr/</guid>
      <description>&lt;p&gt;one paper &lt;a href=&#34;../../publication/conference-paper/cvpr_ma_2025/&#34;&gt;Cinemo&lt;/a&gt; was accpeted by IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)&lt;/p&gt;
&lt;h2 id=&#34;abstract&#34;&gt;Abstract&lt;/h2&gt;
&lt;p&gt;Diffusion models have achieved significant progress in the task of image animation due to their powerful generative capabilities. However, preserving appearance consistency to the static input image, and avoiding abrupt motion change in the generated animation, remains challenging.
In this paper, we introduce Cinemo, a novel image animation approach that aims at achieving better appearance consistency and motion smoothness. The core of Cinemo is to focus on learning the distribution of motion residuals, rather than directly predicting frames as in existing diffusion models.
During the inference, we further mitigate the sudden motion changes in the generated video by introducing a novel DCT-based noise refinement strategy.
To counteract the over-smoothing of motion, we introduce a dynamics degree control design for better control of the magnitude of motion.
Altogether, these strategies enable Cinemo to produce highly consistent, smooth, and motion-controllable results. Extensive experiments compared with several state-of-the-art methods demonstrate the effectiveness and superiority of our proposed approach.
In the end, we also demonstrate how our model can be applied for motion transfer or video editing of any given video. The project page is available at &lt;a href=&#34;https://maxin-cn.github.io/cinemo_project/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://maxin-cn.github.io/cinemo_project/&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;setup&#34;&gt;Setup&lt;/h2&gt;
&lt;p&gt;Download and set up the repo:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;
&lt;pre class=&#34;chroma&#34;&gt;
&lt;code&gt;
git clone https://github.com/maxin-cn/Cinemo
cd Cinemo
conda env create -f environment.yml
conda activate cinemo
&lt;/code&gt;
&lt;/pre&gt;
&lt;/div&gt;
&lt;h2 id=&#34;animation&#34;&gt;Animation&lt;/h2&gt;
&lt;p&gt;You can sample from our pre-trained Cinemo models. Weights for our pre-trained Cinemo model can be found &lt;a href=&#34;https://huggingface.co/maxin-cn/Cinemo/tree/main&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;. The script has various arguments for adjusting sampling steps, changing the classifier-free guidance scale, etc:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;
&lt;pre class=&#34;chroma&#34;&gt;
&lt;code&gt;
bash pipelines/animation.sh
&lt;/code&gt;
&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;Related model weights will be downloaded automatically, and the following results can be found &lt;a href=&#34;https://github.com/maxin-cn/Cinemo/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>üéâ one paper LaVie was accpeted by International Journal of Computer Vision (IJCV)</title>
      <link>https://maxin-cn.github.io/post/lavie-ijcv/</link>
      <pubDate>Mon, 28 Oct 2024 00:00:00 +0000</pubDate>
      <guid>https://maxin-cn.github.io/post/lavie-ijcv/</guid>
      <description>&lt;p&gt;one paper &lt;a href=&#34;../../publication/journal-article/ijcv_lavie_2024/&#34;&gt;LaVie&lt;/a&gt; was accpeted by International Journal of Computer Vision (IJCV)&lt;/p&gt;
&lt;h2 id=&#34;abstract&#34;&gt;Abstract&lt;/h2&gt;
&lt;p&gt;This work aims to learn a high-quality text-to-video (T2V) generative model by leveraging a pre-trained text-to-image (T2I) model as a basis. It is a highly desirable yet challenging task to simultaneously (a) accomplish the synthesis of visually realistic and temporally coherent videos while (b) preserving the strong creative generation nature of the pre-trained T2I model. To this end, we propose LaVie, an integrated video generation framework that operates on cascaded video latent diffusion models, comprising a base T2V model, a temporal interpolation model, and a video super-resolution model. Our key insights are two-fold: (1) We reveal that the incorporation of simple temporal self-attentions, coupled with rotary positional encoding, adequately captures the temporal correlations inherent in video data. (2) Additionally, we validate that the process of joint image-video fine-tuning plays a pivotal role in producing high-quality and creative outcomes. To enhance the performance of LaVie, we contribute a comprehensive and diverse video dataset named Vimeo25M, consisting of 25 million text-video pairs that prioritize quality, diversity, and aesthetic appeal. Extensive experiments demonstrate that LaVie achieves state-of-the-art performance both quantitatively and qualitatively. Furthermore, we showcase the versatility of pre-trained LaVie models in various long video generation and personalized video synthesis applications. Project page: &lt;a href=&#34;https://github.com/Vchitect/LaVie/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/Vchitect/LaVie/&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>üéâ one paper LEO was accpeted by International Journal of Computer Vision (IJCV)</title>
      <link>https://maxin-cn.github.io/post/leo-ijcv/</link>
      <pubDate>Tue, 20 Aug 2024 00:00:00 +0000</pubDate>
      <guid>https://maxin-cn.github.io/post/leo-ijcv/</guid>
      <description>&lt;p&gt;one paper &lt;a href=&#34;../../publication/journal-article/ijcv_leo_2024/&#34;&gt;LEO&lt;/a&gt; was accpeted by International Journal of Computer Vision (IJCV).&lt;/p&gt;
&lt;h2 id=&#34;abstract&#34;&gt;Abstract&lt;/h2&gt;
&lt;p&gt;Spatio-temporal coherency is a major challenge in synthesizing high quality videos, particularly in synthesizing human videos that contain rich global and local deformations. To resolve this challenge, previous approaches have resorted to different features in the generation process aimed at representing appearance and motion. However, in the absence of strict mechanisms to guarantee such disentanglement, a separation of motion from appearance has remained challenging, resulting in spatial distortions and temporal jittering that break the spatio-temporal coherency. Motivated by this, we here propose LEO, a novel framework for human video synthesis, placing emphasis on spatio-temporal coherency. Our key idea is to represent motion as a sequence of flow maps in the generation process, which inherently isolate motion from appearance. We implement this idea via a flow-based image animator and a Latent Motion Diffusion Model (LMDM). The former bridges a space of motion codes with the space of flow maps, and synthesizes video frames in a warp-and-inpaint manner. LMDM learns to capture motion prior in the training data by synthesizing sequences of motion codes. Extensive quantitative and qualitative analysis suggests that LEO significantly improves coherent synthesis of human videos over previous methods on the datasets TaichiHD, FaceForensics and CelebV-HQ. In addition, the effective disentanglement of appearance and motion in LEO allows for two additional tasks, namely infinite-length human video synthesis, as well as content-preserving video editing.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>üë©üèº‚Äçüè´ Invited talk at Intelligent things</title>
      <link>https://maxin-cn.github.io/post/invited-talk-latte/</link>
      <pubDate>Fri, 14 Jun 2024 00:00:00 +0000</pubDate>
      <guid>https://maxin-cn.github.io/post/invited-talk-latte/</guid>
      <description>&lt;p&gt;I have been invited to give a &lt;a href=&#34;../../event&#34;&gt;talk&lt;/a&gt; on &amp;ldquo;Application and Expansion of the DiT Architecture in Video Generation Models&amp;rdquo; at &lt;a href=&#34;https://course.zhidx.com/topic/detail/YmE3M2QxN2I4YmE0M2IwMGQ0MjM=&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Intelligent Things&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;abstract&#34;&gt;Abstract&lt;/h2&gt;
&lt;p&gt;In this talk, I will share the current state and recent advances in video generation research. I will then introduce Latte, a Transformer-based video diffusion model. Following that, I will present some visual comparisons of generated videos. Finally, I will conclude with a discussion on potential future directions and task extensions in text-to-video generation.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>‚úÖ Release the codes of Cinemo</title>
      <link>https://maxin-cn.github.io/post/cinemo-code/</link>
      <pubDate>Sun, 02 Jun 2024 00:00:00 +0000</pubDate>
      <guid>https://maxin-cn.github.io/post/cinemo-code/</guid>
      <description>&lt;p&gt;We release the &lt;a href=&#34;https://github.com/maxin-cn/Cinemo&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;codes&lt;/a&gt; of &lt;a href=&#34;https://arxiv.org/abs/2407.15642&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Cinemo&lt;/a&gt; on Github.&lt;/p&gt;
&lt;h2 id=&#34;setup&#34;&gt;Setup&lt;/h2&gt;
&lt;p&gt;Download and set up the repo:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;
&lt;pre class=&#34;chroma&#34;&gt;
&lt;code&gt;
git clone https://github.com/maxin-cn/Cinemo
cd Cinemo
conda env create -f environment.yml
conda activate cinemo
&lt;/code&gt;
&lt;/pre&gt;
&lt;/div&gt;
&lt;h2 id=&#34;animation&#34;&gt;Animation&lt;/h2&gt;
&lt;p&gt;You can sample from our pre-trained Cinemo models. Weights for our pre-trained Cinemo model can be found &lt;a href=&#34;https://huggingface.co/maxin-cn/Cinemo/tree/main&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;. The script has various arguments for adjusting sampling steps, changing the classifier-free guidance scale, etc:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;
&lt;pre class=&#34;chroma&#34;&gt;
&lt;code&gt;
bash pipelines/animation.sh
&lt;/code&gt;
&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;Related model weights will be downloaded automatically, and the following results can be found &lt;a href=&#34;https://github.com/maxin-cn/Cinemo/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>‚úÖ Release the checkpoints of Latte-1</title>
      <link>https://maxin-cn.github.io/post/latte-code/</link>
      <pubDate>Thu, 23 May 2024 00:00:00 +0000</pubDate>
      <guid>https://maxin-cn.github.io/post/latte-code/</guid>
      <description>&lt;!-- 

&lt;details class=&#34;toc-inpage d-print-none  &#34; open&gt;
  &lt;summary class=&#34;font-weight-bold&#34;&gt;Table of Contents&lt;/summary&gt;
  &lt;nav id=&#34;TableOfContents&#34;&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;a href=&#34;#inference&#34;&gt;Inference&lt;/a&gt;&lt;/li&gt;
  &lt;/ul&gt;
&lt;/nav&gt;
&lt;/details&gt;
 --&gt;
&lt;p&gt;We release the &lt;a href=&#34;https://huggingface.co/maxin-cn/Latte-1&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;chckpoints&lt;/a&gt; of &lt;a href=&#34;https://github.com/Vchitect/Latte&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Latte-1&lt;/a&gt; on Hugging Face.&lt;/p&gt;
&lt;h2 id=&#34;inference&#34;&gt;Inference&lt;/h2&gt;
&lt;p&gt;Latte-1 is now integrated into diffusers.&lt;/p&gt;
&lt;p&gt;You can easily run Latte using the following code. We also support inference with 4/8-bit quantization, which can reduce GPU memory from 17 GB to 9 GB.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;
&lt;pre class=&#34;chroma&#34;&gt;
&lt;code&gt;
#Please update the version of diffusers at leaset to 0.30.0
from diffusers import LattePipeline
from diffusers.models import AutoencoderKLTemporalDecoder
from torchvision.utils import save_image
import torch
import imageio
&lt;p&gt;torch.manual_seed(0)&lt;/p&gt;
&lt;p&gt;device = &amp;ldquo;cuda&amp;rdquo; if torch.cuda.is_available() else &amp;ldquo;cpu&amp;rdquo;
video_length = 16 # 1 (text-to-image) or 16 (text-to-video)
pipe = LattePipeline.from_pretrained(&amp;ldquo;maxin-cn/Latte-1&amp;rdquo;, torch_dtype=torch.float16).to(device)&lt;/p&gt;
&lt;p&gt;#Using temporal decoder of VAE
vae = AutoencoderKLTemporalDecoder.from_pretrained(&amp;ldquo;maxin-cn/Latte-1&amp;rdquo;, subfolder=&amp;ldquo;vae_temporal_decoder&amp;rdquo;, torch_dtype=torch.float16).to(device)
pipe.vae = vae&lt;/p&gt;
&lt;p&gt;prompt = &amp;ldquo;a cat wearing sunglasses and working as a lifeguard at pool.&amp;rdquo;
videos = pipe(prompt, video_length=video_length, output_type=&amp;lsquo;pt&amp;rsquo;).frames.cpu()
&lt;/code&gt;
&lt;/pre&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
