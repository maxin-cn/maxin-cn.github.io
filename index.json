
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    [{"authors":null,"categories":null,"content":"Welcome üëã I am currently a Ph.D candidate at Monash University, supervised by Prof. Tien-Tsin Wong, Prof. Fang-Yuan Li, and Prof. Cunjian Chen. Previously, I received the M.S degree from University of Chinese Academy of Sciences, where I studied at CRIPAC under the leadership of Prof. Tieniu Tan and was supervised by Prof. Ran He. Before that, I obtained the B.E degree from Jiangsu University. My research interests include image super-resolution and inpainting, model compression, face recognition, video generation, large-scale generative models and so on.\n","date":1748822400,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1748822400,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"Welcome üëã I am currently a Ph.D candidate at Monash University, supervised by Prof. Tien-Tsin Wong, Prof. Fang-Yuan Li, and Prof. Cunjian Chen. Previously, I received the M.S degree from University of Chinese Academy of Sciences, where I studied at CRIPAC under the leadership of Prof.","tags":null,"title":"Xin Ma","type":"authors"},{"authors":["Xin Ma"],"categories":null,"content":"I am always pursuse research collaborations. Feel free to contact me if you are interested.\nMy Research Interest I am focus on image super-resolution and inpainting, model compression, face recognition, video generation, large-scale generative models and so on.\n","date":1748822400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1748822400,"objectID":"45bd7fba435739d49cbbfc910cc495d4","permalink":"https://maxin-cn.github.io/post/research-collaboration/","publishdate":"2025-06-02T00:00:00Z","relpermalink":"/post/research-collaboration/","section":"post","summary":"I am always pursuse research collaborations. Feel free to contact me if you are interested.","tags":["Research Collaborations","Markdown"],"title":"üíûÔ∏è Always pursuse research collaborations. Feel free to contact me if you are interested","type":"post"},{"authors":["Xin Ma"],"categories":null,"content":"Our paper OmniPainter was released on arxiv.\nSetup Download and set up the repo:\ngit clone https://github.com/maxin-cn/OmniPainter cd OmniPainter conda env create -f environment.yml conda activate omnipainter Stylized image generation You can sample high-quality images that match both the given prompt and the style reference image within just 4 to 6 timesteps, without requiring any inversion. The script has various arguments for adjusting sampling steps, changing the classifier-free guidance scale, etc:\nbash run.sh Related model weights will be downloaded automatically and following results can be obtained here.\n","date":1748304000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1748304000,"objectID":"e188fb59614c8d2d96498d55bc2c6413","permalink":"https://maxin-cn.github.io/post/omnipainter-code/","publishdate":"2025-05-27T00:00:00Z","relpermalink":"/post/omnipainter-code/","section":"post","summary":"We release OmniPainter on arxiv.","tags":["Arxiv","Markdown"],"title":"‚¨ÜÔ∏è one paper OmniPainter was released on arxiv","type":"post"},{"authors":["Xin Ma","Yaohui Wang","Xinyuan Chen","Tien-Tsin Wong","Cunjian Chen"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software. ","date":1748304000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1748304000,"objectID":"51f6fb3c2fabda14937be0cd1b29c936","permalink":"https://maxin-cn.github.io/publication/preprint/omnipainter/","publishdate":"2025-05-26T00:00:00Z","relpermalink":"/publication/preprint/omnipainter/","section":"publication","summary":"arXiv preprint arXiv:2505.19063, Stars","tags":["Arxiv"],"title":"Training-free Stylized Text-to-Image Generation with Fast Inference","type":"publication"},{"authors":["Xin Ma"],"categories":null,"content":"Our paper Latte was accpeted by Transactions on Machine Learning Research (TMLR)\nAbstract We propose Latte, a novel Latent Diffusion Transformer for video generation. Latte first extracts spatio-temporal tokens from input videos and then adopts a series of Transformer blocks to model video distribution in the latent space. In order to model a substantial number of tokens extracted from videos, four efficient variants are introduced from the perspective of decomposing the spatial and temporal dimensions of input videos. To improve the quality of generated videos, we determine the best practices of Latte through rigorous experimental analysis, including video clip patch embedding, model variants, timestep-class information injection, temporal positional embedding, and learning strategies. Our comprehensive evaluation demonstrates that Latte achieves state-of-the-art performance across four standard video generation datasets, i.e., FaceForensics, SkyTimelapse, UCF101, and Taichi-HD. In addition, we extend Latte to the text-to-video generation (T2V) task, where Latte achieves results that are competitive with recent T2V models. We strongly believe that Latte provides valuable insights for future research on incorporating Transformers into diffusion models for video generation. Project page: https://maxin-cn.github.io/latte_project.\nInference Latte-1 is now integrated into diffusers.\nYou can easily run Latte using the following code. We also support inference with 4/8-bit quantization, which can reduce GPU memory from 17 GB to 9 GB.\n#Please update the version of diffusers at leaset to 0.30.0 from diffusers import LattePipeline from diffusers.models import AutoencoderKLTemporalDecoder from torchvision.utils import save_image import torch import imageio torch.manual_seed(0)\ndevice = ‚Äúcuda‚Äù if torch.cuda.is_available() else ‚Äúcpu‚Äù video_length = 16 # 1 (text-to-image) or 16 (text-to-video) pipe = LattePipeline.from_pretrained(‚Äúmaxin-cn/Latte-1‚Äù, torch_dtype=torch.float16).to(device)\n#Using temporal decoder of VAE vae = AutoencoderKLTemporalDecoder.from_pretrained(‚Äúmaxin-cn/Latte-1‚Äù, subfolder=‚Äúvae_temporal_decoder‚Äù, torch_dtype=torch.float16).to(device) pipe.vae = vae\nprompt = ‚Äúa cat wearing sunglasses and working as a lifeguard at pool.‚Äù videos = pipe(prompt, video_length=video_length, output_type=‚Äòpt‚Äô).frames.cpu() ","date":1742688000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1742688000,"objectID":"d640771b41f4a924961048707a2a341b","permalink":"https://maxin-cn.github.io/post/latte-tmlr/","publishdate":"2025-03-23T00:00:00Z","relpermalink":"/post/latte-tmlr/","section":"post","summary":"Our paper Latte was finally accpeted by Transactions on Machine Learning Research (TMLR)","tags":["TMLR","Markdown"],"title":"üéâ one paper Latte was accpeted by Transactions on Machine Learning Research (TMLR)","type":"post"},{"authors":["Xin Ma"],"categories":null,"content":"one paper Cinemo was accpeted by IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\nAbstract Diffusion models have achieved significant progress in the task of image animation due to their powerful generative capabilities. However, preserving appearance consistency to the static input image, and avoiding abrupt motion change in the generated animation, remains challenging. In this paper, we introduce Cinemo, a novel image animation approach that aims at achieving better appearance consistency and motion smoothness. The core of Cinemo is to focus on learning the distribution of motion residuals, rather than directly predicting frames as in existing diffusion models. During the inference, we further mitigate the sudden motion changes in the generated video by introducing a novel DCT-based noise refinement strategy. To counteract the over-smoothing of motion, we introduce a dynamics degree control design for better control of the magnitude of motion. Altogether, these strategies enable Cinemo to produce highly consistent, smooth, and motion-controllable results. Extensive experiments compared with several state-of-the-art methods demonstrate the effectiveness and superiority of our proposed approach. In the end, we also demonstrate how our model can be applied for motion transfer or video editing of any given video. The project page is available at https://maxin-cn.github.io/cinemo_project/.\nSetup Download and set up the repo:\ngit clone https://github.com/maxin-cn/Cinemo cd Cinemo conda env create -f environment.yml conda activate cinemo Animation You can sample from our pre-trained Cinemo models. Weights for our pre-trained Cinemo model can be found here. The script has various arguments for adjusting sampling steps, changing the classifier-free guidance scale, etc:\nbash pipelines/animation.sh Related model weights will be downloaded automatically, and the following results can be found here.\n","date":1740614400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1740614400,"objectID":"03d8a1b9fa11a0904d5db063eec01d55","permalink":"https://maxin-cn.github.io/post/cinemo-cvpr/","publishdate":"2025-02-27T00:00:00Z","relpermalink":"/post/cinemo-cvpr/","section":"post","summary":"Our paper Cinemo was finally accpeted by IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","tags":["CVPR","Markdown"],"title":"üéâ one paper Cinemo was accpeted by IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","type":"post"},{"authors":["Xin Ma"],"categories":null,"content":"one paper LaVie was accpeted by International Journal of Computer Vision (IJCV)\nAbstract This work aims to learn a high-quality text-to-video (T2V) generative model by leveraging a pre-trained text-to-image (T2I) model as a basis. It is a highly desirable yet challenging task to simultaneously (a) accomplish the synthesis of visually realistic and temporally coherent videos while (b) preserving the strong creative generation nature of the pre-trained T2I model. To this end, we propose LaVie, an integrated video generation framework that operates on cascaded video latent diffusion models, comprising a base T2V model, a temporal interpolation model, and a video super-resolution model. Our key insights are two-fold: (1) We reveal that the incorporation of simple temporal self-attentions, coupled with rotary positional encoding, adequately captures the temporal correlations inherent in video data. (2) Additionally, we validate that the process of joint image-video fine-tuning plays a pivotal role in producing high-quality and creative outcomes. To enhance the performance of LaVie, we contribute a comprehensive and diverse video dataset named Vimeo25M, consisting of 25 million text-video pairs that prioritize quality, diversity, and aesthetic appeal. Extensive experiments demonstrate that LaVie achieves state-of-the-art performance both quantitatively and qualitatively. Furthermore, we showcase the versatility of pre-trained LaVie models in various long video generation and personalized video synthesis applications. Project page: https://github.com/Vchitect/LaVie/.\n","date":1730073600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1730073600,"objectID":"0cace8e943cc12f7626dd6c1849a76e7","permalink":"https://maxin-cn.github.io/post/lavie-ijcv/","publishdate":"2024-10-28T00:00:00Z","relpermalink":"/post/lavie-ijcv/","section":"post","summary":"Our paper LaVie was finally accpeted by International Journal of Computer Vision (IJCV)","tags":["IJCV","Markdown"],"title":"üéâ one paper LaVie was accpeted by International Journal of Computer Vision (IJCV)","type":"post"},{"authors":["Xin Ma"],"categories":null,"content":"one paper LEO was accpeted by International Journal of Computer Vision (IJCV).\nAbstract Spatio-temporal coherency is a major challenge in synthesizing high quality videos, particularly in synthesizing human videos that contain rich global and local deformations. To resolve this challenge, previous approaches have resorted to different features in the generation process aimed at representing appearance and motion. However, in the absence of strict mechanisms to guarantee such disentanglement, a separation of motion from appearance has remained challenging, resulting in spatial distortions and temporal jittering that break the spatio-temporal coherency. Motivated by this, we here propose LEO, a novel framework for human video synthesis, placing emphasis on spatio-temporal coherency. Our key idea is to represent motion as a sequence of flow maps in the generation process, which inherently isolate motion from appearance. We implement this idea via a flow-based image animator and a Latent Motion Diffusion Model (LMDM). The former bridges a space of motion codes with the space of flow maps, and synthesizes video frames in a warp-and-inpaint manner. LMDM learns to capture motion prior in the training data by synthesizing sequences of motion codes. Extensive quantitative and qualitative analysis suggests that LEO significantly improves coherent synthesis of human videos over previous methods on the datasets TaichiHD, FaceForensics and CelebV-HQ. In addition, the effective disentanglement of appearance and motion in LEO allows for two additional tasks, namely infinite-length human video synthesis, as well as content-preserving video editing.\n","date":1724112000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1724112000,"objectID":"f4d508151a335c064143904515349f55","permalink":"https://maxin-cn.github.io/post/leo-ijcv/","publishdate":"2024-08-20T00:00:00Z","relpermalink":"/post/leo-ijcv/","section":"post","summary":"Our paper LEO was finally accpeted by International Journal of Computer Vision (IJCV)","tags":["IJCV","Markdown"],"title":"üéâ one paper LEO was accpeted by International Journal of Computer Vision (IJCV)","type":"post"},{"authors":["Xin Ma","Yaohui Wang","Gengyun Jia","Xinyuan Chen","Tien-Tsin Wong","Yuan-Fang Li","Cunjian Chen"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software. ","date":1721606400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721606400,"objectID":"5acfebc2b4d64639388a614a24d13e11","permalink":"https://maxin-cn.github.io/publication/conference-paper/cvpr_ma_2025/","publishdate":"2024-07-22T00:00:00Z","relpermalink":"/publication/conference-paper/cvpr_ma_2025/","section":"publication","summary":"Published in Computer Vision and Pattern Recognition (CVPR), 2025, CCF-A, Stars","tags":["CVPR"],"title":"Consistent and Controllable Image Animation with Motion Diffusion Models","type":"publication"},{"authors":null,"categories":null,"content":"","date":1718409600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1718409600,"objectID":"e0bef30d10398c323f9d4183403ccadc","permalink":"https://maxin-cn.github.io/activities/","publishdate":"2024-06-15T00:00:00Z","relpermalink":"/activities/","section":"","summary":"","tags":null,"title":"Activity","type":"landing"},{"authors":null,"categories":null,"content":"","date":1718409600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1718409600,"objectID":"3a94d5b22596e839a64d8310d4069067","permalink":"https://maxin-cn.github.io/activity/","publishdate":"2024-06-15T00:00:00Z","relpermalink":"/activity/","section":"","summary":"","tags":null,"title":"Activity","type":"landing"},{"authors":null,"categories":null,"content":"","date":1718409600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1718409600,"objectID":"3c4864f00d23f7ea35511ec930ce1d9c","permalink":"https://maxin-cn.github.io/contact/","publishdate":"2024-06-15T00:00:00Z","relpermalink":"/contact/","section":"","summary":"","tags":null,"title":"Contact","type":"landing"},{"authors":null,"categories":null,"content":"","date":1718409600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1718409600,"objectID":"369971e71f1c506c9ca3ffab10099561","permalink":"https://maxin-cn.github.io/patent/","publishdate":"2024-06-15T00:00:00Z","relpermalink":"/patent/","section":"","summary":"","tags":null,"title":"Patent","type":"landing"},{"authors":null,"categories":null,"content":"","date":1718409600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1718409600,"objectID":"a7d933e1728d7cdba4b96b3eb7efd85e","permalink":"https://maxin-cn.github.io/selected_projects/","publishdate":"2024-06-15T00:00:00Z","relpermalink":"/selected_projects/","section":"","summary":"","tags":null,"title":"Project","type":"landing"},{"authors":null,"categories":null,"content":"","date":1718409600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1718409600,"objectID":"480c4de99851329b51acecc000e2e84f","permalink":"https://maxin-cn.github.io/publications/","publishdate":"2024-06-15T00:00:00Z","relpermalink":"/publications/","section":"","summary":"","tags":null,"title":"Publication","type":"landing"},{"authors":null,"categories":null,"content":"","date":1718409600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1718409600,"objectID":"49cbbfcf1124f111a37fcbac0faeb48e","permalink":"https://maxin-cn.github.io/talks/","publishdate":"2024-06-15T00:00:00Z","relpermalink":"/talks/","section":"","summary":"","tags":null,"title":"Talks","type":"landing"},{"authors":[],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature. ","date":1718359200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1718359200,"objectID":"4e352c7c13f6ee178d031b4ea0c57fef","permalink":"https://maxin-cn.github.io/talk/application-and-expansion-of-dit-architecture-in-video-generation-models/","publishdate":"2023-06-14T14:00:00Z","relpermalink":"/talk/application-and-expansion-of-dit-architecture-in-video-generation-models/","section":"event","summary":"A talk introducing Latte.","tags":[],"title":"Application and expansion of DiT architecture in video generation models","type":"event"},{"authors":["Xin Ma"],"categories":null,"content":"I have been invited to give a talk on ‚ÄúApplication and Expansion of the DiT Architecture in Video Generation Models‚Äù at Intelligent Things.\nAbstract In this talk, I will share the current state and recent advances in video generation research. I will then introduce Latte, a Transformer-based video diffusion model. Following that, I will present some visual comparisons of generated videos. Finally, I will conclude with a discussion on potential future directions and task extensions in text-to-video generation.\n","date":1718323200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1718323200,"objectID":"c08870a5325e11d33852d7c8d5d0621d","permalink":"https://maxin-cn.github.io/post/invited-talk-latte/","publishdate":"2024-06-14T00:00:00Z","relpermalink":"/post/invited-talk-latte/","section":"post","summary":"I have been invited to give a talk on \"Application and Expansion of the DiT Architecture in Video Generation Models\" at Intelligent Things.","tags":["Intelligent things","Markdown"],"title":"üë©üèº‚Äçüè´ Invited talk on ‚ÄúApplication and expansion of DiT architecture in video generation models‚Äù at Intelligent things","type":"post"},{"authors":["Xin Ma"],"categories":null,"content":"We release the codes of Cinemo on Github.\nSetup Download and set up the repo:\ngit clone https://github.com/maxin-cn/Cinemo cd Cinemo conda env create -f environment.yml conda activate cinemo Animation You can sample from our pre-trained Cinemo models. Weights for our pre-trained Cinemo model can be found here. The script has various arguments for adjusting sampling steps, changing the classifier-free guidance scale, etc:\nbash pipelines/animation.sh Related model weights will be downloaded automatically, and the following results can be found here.\n","date":1717286400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1717286400,"objectID":"030c20f56405d7ab3bf71a72213d72e5","permalink":"https://maxin-cn.github.io/post/cinemo-code/","publishdate":"2024-06-02T00:00:00Z","relpermalink":"/post/cinemo-code/","section":"post","summary":"We release the Cinemo codes on Github.","tags":["Github","Markdown"],"title":"‚úÖ Release the codes of Cinemo","type":"post"},{"authors":null,"categories":null,"content":"","date":1716595200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1716595200,"objectID":"f76fd61ffc133f7669794a93836b7c5d","permalink":"https://maxin-cn.github.io/project/cinemo/","publishdate":"2024-05-25T00:00:00Z","relpermalink":"/project/cinemo/","section":"project","summary":"This framework offers simpler, more precise user control and better image animation performance.","tags":["Image Animation","Diffusion"],"title":"Cinemo - An image animation Framework (CVPR 2025)","type":"project"},{"authors":["Xin Ma"],"categories":null,"content":" We release the chckpoints of Latte-1 on Hugging Face.\nInference Latte-1 is now integrated into diffusers.\nYou can easily run Latte using the following code. We also support inference with 4/8-bit quantization, which can reduce GPU memory from 17 GB to 9 GB.\n#Please update the version of diffusers at leaset to 0.30.0 from diffusers import LattePipeline from diffusers.models import AutoencoderKLTemporalDecoder from torchvision.utils import save_image import torch import imageio torch.manual_seed(0)\ndevice = ‚Äúcuda‚Äù if torch.cuda.is_available() else ‚Äúcpu‚Äù video_length = 16 # 1 (text-to-image) or 16 (text-to-video) pipe = LattePipeline.from_pretrained(‚Äúmaxin-cn/Latte-1‚Äù, torch_dtype=torch.float16).to(device)\n#Using temporal decoder of VAE vae = AutoencoderKLTemporalDecoder.from_pretrained(‚Äúmaxin-cn/Latte-1‚Äù, subfolder=‚Äúvae_temporal_decoder‚Äù, torch_dtype=torch.float16).to(device) pipe.vae = vae\nprompt = ‚Äúa cat wearing sunglasses and working as a lifeguard at pool.‚Äù videos = pipe(prompt, video_length=video_length, output_type=‚Äòpt‚Äô).frames.cpu() ","date":1716422400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1716422400,"objectID":"cb8397859577b8634f38fbc63f04eb37","permalink":"https://maxin-cn.github.io/post/latte-code/","publishdate":"2024-05-23T00:00:00Z","relpermalink":"/post/latte-code/","section":"post","summary":"We release the chckpoints of Latte-1 on Hugging Face.","tags":["Hugging Face","Markdown"],"title":"‚úÖ Release the checkpoints of Latte-1","type":"post"},{"authors":["Yi Wang","Yinan He","Yizhuo Li","Kunchang Li","Jiashuo Yu","Xin Ma","Xinhao Li","Guo Chen","Xinyuan Chen","Yaohui Wang","Conghui He","Ping Luo","Ziwei Liu","Yali Wang","Limin Wang","Yu Qiao"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software. ","date":1705366500,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1705366500,"objectID":"82d4c8f3405a0c72c4926a4e1febf010","permalink":"https://maxin-cn.github.io/publication/conference-paper/iclr_wang_2024/","publishdate":"2024-01-16T00:55:00Z","relpermalink":"/publication/conference-paper/iclr_wang_2024/","section":"publication","summary":"Published in International Conference on Learning Representations (ICLR), 2024, Stars","tags":["ICLR"],"title":"InternVid: A Large-scale Video-Text Dataset for Multimodal Understanding and Generation","type":"publication"},{"authors":["Xinyuan Chen","Yaohui Wang","Lingjun Zhang","Shaobin Zhuang","Xin Ma","Jiashuo Yu","Yali Wang","Dahua Lin","Yu Qiao","Ziwei Liu"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software. ","date":1705365360,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1705365360,"objectID":"023b44e626932117d58f0e7054b2b548","permalink":"https://maxin-cn.github.io/publication/conference-paper/iclr_chen_2024/","publishdate":"2024-01-16T00:36:00Z","relpermalink":"/publication/conference-paper/iclr_chen_2024/","section":"publication","summary":"Published in International Conference on Learning Representations (ICLR), 2024, Stars","tags":["ICLR"],"title":"SEINE: Short-to-Long Video Diffusion Model for Generative Transition and Prediction","type":"publication"},{"authors":["Xin Ma","Yaohui Wang","Xinyuan Chen","Gengyun Jia","Ziwei Liu","Yuan-Fang Li","Cunjian Chen","Yu Qiao"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software. ","date":1704412800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1704412800,"objectID":"8b4436d96088bb97d1bb95fa23376cd6","permalink":"https://maxin-cn.github.io/publication/journal-article/tmlr_latte_2025/","publishdate":"2024-01-05T00:00:00Z","relpermalink":"/publication/journal-article/tmlr_latte_2025/","section":"publication","summary":"Published in Transactions on Machine Learning Research (TMLR), 2025, Stars","tags":["TMLR"],"title":"Latte: Latent Diffusion Transformer for Video Generation","type":"publication"},{"authors":null,"categories":null,"content":"","date":1698105600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1698105600,"objectID":"38626dc0bfb86d4577bc2893e4a8a7b1","permalink":"https://maxin-cn.github.io/work_experiences/","publishdate":"2023-10-24T00:00:00Z","relpermalink":"/work_experiences/","section":"","summary":"","tags":null,"title":"Experience","type":"landing"},{"authors":null,"categories":null,"content":"","date":1695859200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1695859200,"objectID":"71989c222514edbdb50fd42362ea0372","permalink":"https://maxin-cn.github.io/project/latte/","publishdate":"2023-09-28T00:00:00Z","relpermalink":"/project/latte/","section":"project","summary":"A simple and general latent video diffusion model incorporating sptio-temporal Transformers for video generation.","tags":["Text-to-video Generation","Transformers"],"title":"Latte - A first open source Transformer-based Video Diffusion Generation Framework (TMLR 2025)","type":"project"},{"authors":null,"categories":null,"content":"","date":1695772800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1695772800,"objectID":"e95e3430044ac4a4f6735152222621f0","permalink":"https://maxin-cn.github.io/project/lavie/","publishdate":"2023-09-27T00:00:00Z","relpermalink":"/project/lavie/","section":"project","summary":"A large-scale text-to-video framework that produces high-quality and temporally coherent videos. This framework operates on cascaded video latent diffusion models, comprising a base T2V model, a temporal interpolation model, and a video super-resolution model.","tags":["Text-to-video Generation"],"title":"LaVie - A High-Quality Video Generation Framework (IJCV 2024)","type":"project"},{"authors":["Xin Ma","Yaohui Wang","Xinyuan Chen","Xin Ma","Shangchen Zhou","Ziqi Huang","Yi Wang","Ceyuan Yang","Yinan He","Jiashuo Yu","Peiqing Yang","Yuwei Guo","Tianxing Wu","Chenyang Si","Yuming Jiang","Cunjian Chen","Chen Change Loy","Bo Dai","Dahua Lin","Yu Qiao","Ziwei Liu"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software. ","date":1695686400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1695686400,"objectID":"8bf79053b14b853764d1fe5628f84773","permalink":"https://maxin-cn.github.io/publication/journal-article/ijcv_lavie_2024/","publishdate":"2023-09-26T00:00:00Z","relpermalink":"/publication/journal-article/ijcv_lavie_2024/","section":"publication","summary":"Published in International Journal of Computer Vision (IJCV), 2024, JCR Q1 \u0026 CCF-A, Stars","tags":["IJCV"],"title":"LaVie: High-Quality Video Generation with Cascaded Latent Diffusion Models","type":"publication"},{"authors":["Xin Ma","Xiaoqiang Zhou","Huaibo Huang","Gengyun Jia","Yaohui Wang","Xinyuan Chen","Cunjian Chen"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software. ","date":1691539200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1691539200,"objectID":"7ee18d635976c67d5dda344ba3b71914","permalink":"https://maxin-cn.github.io/publication/journal-article/eswa_2023/","publishdate":"2023-08-09T00:00:00Z","relpermalink":"/publication/journal-article/eswa_2023/","section":"publication","summary":"Published in Expert Systems with Applications (ESWA), 2023, JCR Q1 \u0026 CCF-C, Stars","tags":["ESWA"],"title":"Uncertainty-Aware Image Inpainting with Adaptive Feedback Network","type":"publication"},{"authors":["Yaohui Wang","Xin Ma","Xinyuan Chen","Cunjian Chen","Antitza Dantcheva","Bo Dai","Yu Qiao"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software. ","date":1683331200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1683331200,"objectID":"75000dd0a22d1379ccc22062b73e916c","permalink":"https://maxin-cn.github.io/publication/journal-article/ijcv_leo_2024/","publishdate":"2024-08-20T00:00:00Z","relpermalink":"/publication/journal-article/ijcv_leo_2024/","section":"publication","summary":"Published in International Journal of Computer Vision (IJCV), 2024, JCR Q1 \u0026 CCF-A","tags":["IJCV"],"title":"LEO: Generative Latent Image Animator for Human Video Synthesis","type":"publication"},{"authors":["Mandi Luo","Xin Ma","Huaibo Huang","Ran He"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software. ","date":1656633600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1656633600,"objectID":"7fc29e754d4db4dcd5a9d45c1a1606e2","permalink":"https://maxin-cn.github.io/publication/conference-paper/prcv_2022/","publishdate":"2022-07-01T00:00:00Z","relpermalink":"/publication/conference-paper/prcv_2022/","section":"publication","summary":"Published in Pattern Recognition and Computer Vision (PRCV), 2022, CCF-C","tags":["PRCV"],"title":"Style-Based Attentive Network for Real-World Face Hallucination","type":"publication"},{"authors":null,"categories":null,"content":"","date":1655251200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1655251200,"objectID":"fd2b179a695644f4041323bf1f73740f","permalink":"https://maxin-cn.github.io/bak_index/","publishdate":"2022-06-15T00:00:00Z","relpermalink":"/bak_index/","section":"","summary":"","tags":null,"title":"","type":"landing"},{"authors":["Huanyu Wang","Junjie Liu","Xin Ma","Yang Yong","Zhenhua Chai","Jianxin Wu"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software. ","date":1651363200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1651363200,"objectID":"c06e5c616a07a4b9cc8492b547f093a8","permalink":"https://maxin-cn.github.io/publication/conference-paper/cvpr_2022/","publishdate":"2022-05-01T00:00:00Z","relpermalink":"/publication/conference-paper/cvpr_2022/","section":"publication","summary":"Published in Computer Vision and Pattern Recognition (CVPR), 2022, CCF-A","tags":["CVPR"],"title":"Compressing Models with Few Samples: Mimicking then Replacing","type":"publication"},{"authors":["Xin Ma","Xiaoqiang Zhou","Huaibo Huang","Gengyun Jia","Zhenhua Chai","Xiaolin Wei"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software. ","date":1637971200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1637971200,"objectID":"ef966bb40071611b8a61dbda2f92d6cd","permalink":"https://maxin-cn.github.io/publication/journal-article/pr_2022/","publishdate":"2021-11-27T00:00:00Z","relpermalink":"/publication/journal-article/pr_2022/","section":"publication","summary":"Published in Pattern Recognition (PR), 2022, JCR Q1 \u0026 CCF-B","tags":["PR"],"title":"Contrastive attention network with dense field estimation for face completion","type":"publication"},{"authors":["Mandi Luo","Xin Ma","Zhihang Li","Jie Cao","Ran He"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software. ","date":1635292800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635292800,"objectID":"318acef0f1616461feed18b6f5d10f77","permalink":"https://maxin-cn.github.io/publication/journal-article/tifs_partial_2021/","publishdate":"2021-10-27T00:00:00Z","relpermalink":"/publication/journal-article/tifs_partial_2021/","section":"publication","summary":"Published in IEEE Transactions on Information Forensics and Security (T-IFS), 2021, JCR Q1 \u0026 CCF-A","tags":["TIFS"],"title":"Partial NIR-VIS heterogeneous face recognition with automatic saliency search","type":"publication"},{"authors":["Xin Ma","Xiaoqiang Zhou","Huaibo Huang","Zhenhua Chai","Xiaolin Wei","Ran He"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software. ","date":1633046400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1633046400,"objectID":"ea8eaf492f9c0c0a56b15e6685f11f4c","permalink":"https://maxin-cn.github.io/publication/conference-paper/icpr_ma_2021/","publishdate":"2021-10-01T00:00:00Z","relpermalink":"/publication/conference-paper/icpr_ma_2021/","section":"publication","summary":"Published in International Conference on Pattern Recognition (ICPR), 2021, CCF-C","tags":["ICPR"],"title":"Free-form image inpainting via contrastive attention network","type":"publication"},{"authors":["Yuhe Ding","Xin Ma","Mandi Luo","Aihua Zheng","Ran He"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software. ","date":1633046400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1633046400,"objectID":"4d61fb7539c1a19617e81930ed6f45fa","permalink":"https://maxin-cn.github.io/publication/conference-paper/icpr_ding_2021/","publishdate":"2021-10-01T00:00:00Z","relpermalink":"/publication/conference-paper/icpr_ding_2021/","section":"publication","summary":"Published in International Conference on Pattern Recognition (ICPR), 2021, CCF-C","tags":["ICPR"],"title":"Unsupervised Contrastive Photo-to-Caricature Translation based on Auto-distortion","type":"publication"},{"authors":["Gengyun Jia","Meisong Zheng","Chuanrui Hu","Xin Ma","Yuting Xu","Luoqi Liu","Yafeng Deng","Ran He"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software. ","date":1622505600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1622505600,"objectID":"bc3d6a801101e01faed22698ee7479a4","permalink":"https://maxin-cn.github.io/publication/journal-article/tbiom_2021/","publishdate":"2021-06-01T00:00:00Z","relpermalink":"/publication/journal-article/tbiom_2021/","section":"publication","summary":"Published in IEEE Transactions on Biometrics, Behavior, and Identity Science (T-BIOM), 2021","tags":["TBIOM"],"title":"Inconsistency-aware wavelet dual-branch network for face forgery detection","type":"publication"},{"authors":["Mandi Luo","Jie Cao","Xin Ma","Xiaoyu Zhang","Ran He"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software. ","date":1611705600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1611705600,"objectID":"a2542ed346f0643f1a47e707f1f0512d","permalink":"https://maxin-cn.github.io/publication/journal-article/tifs_fagan_2021/","publishdate":"2021-01-27T00:00:00Z","relpermalink":"/publication/journal-article/tifs_fagan_2021/","section":"publication","summary":"Published in IEEE Transactions on Information Forensics and Security (T-IFS), 2021, JCR Q1 \u0026 CCF-A","tags":["TIFS"],"title":"FA-GAN: face augmentation GAN for deformation-invariant face recognition","type":"publication"}]