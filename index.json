
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    [{"authors":null,"categories":null,"content":"Welcome üëã I am currently a Ph.D candidate at Monash University, supervised by Prof. Tien-Tsin Wong, Prof. Fang-Yuan Li, and Prof. Cunjian Chen. Previously, I received the M.S degree from University of Chinese Academy of Sciences, where I studied at CRIPAC under the leadership of Prof. Tieniu Tan and was supervised by Prof. Ran He. My research interests include video and image generation, model compression, face recognition, large-scale generative models and so on.\n","date":1759276800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1759276800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"Welcome üëã I am currently a Ph.D candidate at Monash University, supervised by Prof. Tien-Tsin Wong, Prof. Fang-Yuan Li, and Prof. Cunjian Chen. Previously, I received the M.S degree from University of Chinese Academy of Sciences, where I studied at CRIPAC under the leadership of Prof.","tags":null,"title":"Xin Ma","type":"authors"},{"authors":["Jian Lin","Chengze Li","Haoyun Qin","Hanyuan Liu","Xueting Liu","Xin Ma","Cunjian Chen","Tien-Tsin Wong"],"categories":null,"content":" ","date":1759276800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1759276800,"objectID":"b3434f2a29413de87dfa874ae78a8784","permalink":"https://maxin-cn.github.io/publication/conference-paper/pg_2025/","publishdate":"2025-10-01T00:00:00Z","relpermalink":"/publication/conference-paper/pg_2025/","section":"publication","summary":"Published in Pacific Conference on Computer Graphics and Applications (PG), 2025","tags":["PG"],"title":"Trajectory-guided Anime Video Synthesis via Effective Motion Learning","type":"publication"},{"authors":["Xin Ma"],"categories":null,"content":" We have released MiraMo on arXiv. It is among the first papers to explore linear attention for video generation.\nSetup Download and set up the repo:\ngit clone https://github.com/maxin-cn/MiraMo cd MiraMo conda env create -f environment.yml conda activate miramo Animation You can sample from our pre-trained MiraMo models. Weights for our pre-trained MiraMo model can be found here. The script has various arguments for adjusting sampling steps, changing the classifier-free guidance scale, etc:\nbash pipelines/animation.sh Related model weights will be downloaded automatically, and the following results can be found here.\n","date":1754784000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1754784000,"objectID":"e19835cb6aba7314245ac277859eb0eb","permalink":"https://maxin-cn.github.io/post/miramo-paper/","publishdate":"2025-08-10T00:00:00Z","relpermalink":"/post/miramo-paper/","section":"post","summary":"We have released MiraMO on arXiv. It is among the first papers to explore linear attention for video generation.","tags":["arXiv","Markdown"],"title":"‚¨ÜÔ∏è one paper Miramo exploring image animation using linear attention was released on arXiv","type":"post"},{"authors":["Xin Ma","Yaohui Wang","Jia Gengyun","Xinyuan Chen","Tien-Tsin Wong","Cunjian Chen"],"categories":null,"content":"\r","date":1754784000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1754784000,"objectID":"59d9129054fa31519c56d62a45fba839","permalink":"https://maxin-cn.github.io/publication/preprint/miramo/","publishdate":"2025-08-10T00:00:00Z","relpermalink":"/publication/preprint/miramo/","section":"publication","summary":"arXiv preprint arXiv:2508.07246, Stars","tags":["Arxiv"],"title":"Consistent and Controllable Image Animation with Motion Linear Diffusion Transformers","type":"publication"},{"authors":["Xin Ma"],"categories":null,"content":"Our paper, titled ‚ÄúTrajectory-guided Anime Video Synthesis via Effective Motion Learning‚Äù, which focuses on cartoon and anime motion production, has been accepted to Pacific Graphics (PG).\nAbstract Cartoon and anime motion production is traditionally labor-intensive, requiring detailed animatics and extensive in-betweening from keyframes. To streamline this process, we propose a novel framework that synthesizes motion directly from a single colored keyframe, guided by user-provided trajectories. Addressing the limitations of prior methods, which struggle with anime due to reliance on optical flow estimators and models trained on natural videos, we introduce an efficient motion representation specifically adapted for anime, leveraging CoTracker to capture sparse frame-to-frame tracking effectively. To achieve our objective, we design a two-stage learning mechanism: the first stage predicts sparse motion from input frames and trajectories, generating a motion preview sequence via explicit warping; the second stage refines these previews into high-quality anime frames by fine-tuning ToonCrafter, an anime-specific video diffusion model. We train our framework on a novel animation video dataset comprising more than 500,000 clips. Experimental results demonstrate significant improvements in animating still frames, achieving better alignment with user-provided trajectories and more natural motion patterns while preserving anime stylization and visual quality. Our method also supports versatile applications, including motion manga generation and 2D vector graphic animations.\n","date":1754697600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1754697600,"objectID":"fd20e839bb254839488ed662661ff574","permalink":"https://maxin-cn.github.io/post/cartoon-pg2025/","publishdate":"2025-08-09T00:00:00Z","relpermalink":"/post/cartoon-pg2025/","section":"post","summary":"Our paper, titled ‚ÄúTrajectory-guided Anime Video Synthesis via Effective Motion Learning‚Äù, which focuses on cartoon and anime motion production, has been accepted to Pacific Conference on Computer Graphics and Applications (PG) 2025.","tags":["PG"],"title":"üéâ one paper for the cartoon animation was accpeted by Pacific Conference on Computer Graphics and Applications (PG) 2025","type":"post"},{"authors":["Xin Ma"],"categories":null,"content":"I am expected to graduate between late 2025 and early 2026, and I am currently seeking full-time opportunities. If you are interested, please contact me. Many thanks~\nMy Research Interest I am focus on video and image generation, multimodal models, low-level vision, and face recognition, among others.\n","date":1752105600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1752105600,"objectID":"82b8bffa049631d487c91004443bd095","permalink":"https://maxin-cn.github.io/post/looking-job/","publishdate":"2025-07-10T00:00:00Z","relpermalink":"/post/looking-job/","section":"post","summary":"I am currently seeking full-time job opportunities. If you are interested, please contact me. Thanks!","tags":["Job opportunities"],"title":"üë©üèº‚Äçüè´ Graduating in late 2025 to early 2026 and currently seeking full-time job opportunities. Please contact me if you are recruiting","type":"post"},{"authors":["Xin Ma"],"categories":null,"content":"I am honored to have been selected as an Outstanding Reviewer by the CVPR 2025 committee. Earlier, I was also awarded the CVPR 2025 Travel Support Award.\nDetails of the Outstanding Reviewers for CVPR 2025 This recognition reflects not only the quality and depth of the reviews but also the commitment to fairness, clarity, and timely feedback throughout the review process. A total of 12,593 reviewers submitted reviews for CVPR 2025. A total of 711 outstanding reviewers are chosen from reviewers who have submitted at least 3 reviews and are highly rated by area chairs. Total list of outstanding reviewers is available here.\nDetails of the Travel Support Award for CVPR 2025 The CVPR 2025 Travel Support Award provides partial coverage for travel expenses and waives the registration fee.\n","date":1749772800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1749772800,"objectID":"c522d78c0a7fe388a18efa96e9847397","permalink":"https://maxin-cn.github.io/post/outstanding-reviewer-cvpr2025/","publishdate":"2025-06-13T00:00:00Z","relpermalink":"/post/outstanding-reviewer-cvpr2025/","section":"post","summary":"I'm honored to have been selected as an Outstanding Reviewer by the CVPR 2025 committee. Earlier, I was also awarded the CVPR 2025 Travel Support Award.","tags":["Outstanding Reviewers","Travel Support Award"],"title":"üéâ I was recognized as the outstanding reviewer for CVPR 2025 and won the CVPR 2025 Travel Support Award","type":"post"},{"authors":["Xin Ma"],"categories":null,"content":"I am always pursuse research collaborations. Feel free to contact me if you are interested.\nMy Research Interest I am focus on video and image generation, multimodal models, low-level vision, and face recognition, among others.\n","date":1748822400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1748822400,"objectID":"45bd7fba435739d49cbbfc910cc495d4","permalink":"https://maxin-cn.github.io/post/research-collaboration/","publishdate":"2025-06-02T00:00:00Z","relpermalink":"/post/research-collaboration/","section":"post","summary":"I am always pursuse research collaborations. Feel free to contact me if you are interested.","tags":["Research Collaborations","Markdown"],"title":"üíûÔ∏è Always pursuse research collaborations. Feel free to contact me if you are interested","type":"post"},{"authors":["Xin Ma"],"categories":null,"content":"Our paper OmniPainter for the sylized text-to-image generation was released on arxiv.\nSetup Download and set up the repo:\ngit clone https://github.com/maxin-cn/OmniPainter cd OmniPainter conda env create -f environment.yml conda activate omnipainter Stylized image generation You can sample high-quality images that match both the given prompt and the style reference image within just 4 to 6 timesteps, without requiring any inversion. The script has various arguments for adjusting sampling steps, changing the classifier-free guidance scale, etc:\nbash run.sh Related model weights will be downloaded automatically and following results can be obtained here.\n","date":1748304000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1748304000,"objectID":"e188fb59614c8d2d96498d55bc2c6413","permalink":"https://maxin-cn.github.io/post/omnipainter-code/","publishdate":"2025-05-27T00:00:00Z","relpermalink":"/post/omnipainter-code/","section":"post","summary":"We release OmniPainter on arXiv.","tags":["arXiv","Markdown"],"title":"‚¨ÜÔ∏è one paper OmniPainter for the sylized T2I generation was released on arXiv","type":"post"},{"authors":["Xin Ma","Yaohui Wang","Xinyuan Chen","Tien-Tsin Wong","Cunjian Chen"],"categories":null,"content":"\r","date":1748304000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1748304000,"objectID":"51f6fb3c2fabda14937be0cd1b29c936","permalink":"https://maxin-cn.github.io/publication/preprint/omnipainter/","publishdate":"2025-05-26T00:00:00Z","relpermalink":"/publication/preprint/omnipainter/","section":"publication","summary":"arXiv preprint arXiv:2505.19063, Stars","tags":["Arxiv"],"title":"Training-free Stylized Text-to-Image Generation with Fast Inference","type":"publication"},{"authors":["Xin Ma"],"categories":null,"content":"Our paper Latte was accpeted by Transactions on Machine Learning Research (TMLR)\nAbstract We propose Latte, a novel Latent Diffusion Transformer for video generation. Latte first extracts spatio-temporal tokens from input videos and then adopts a series of Transformer blocks to model video distribution in the latent space. In order to model a substantial number of tokens extracted from videos, four efficient variants are introduced from the perspective of decomposing the spatial and temporal dimensions of input videos. To improve the quality of generated videos, we determine the best practices of Latte through rigorous experimental analysis, including video clip patch embedding, model variants, timestep-class information injection, temporal positional embedding, and learning strategies. Our comprehensive evaluation demonstrates that Latte achieves state-of-the-art performance across four standard video generation datasets, i.e., FaceForensics, SkyTimelapse, UCF101, and Taichi-HD. In addition, we extend Latte to the text-to-video generation (T2V) task, where Latte achieves results that are competitive with recent T2V models. We strongly believe that Latte provides valuable insights for future research on incorporating Transformers into diffusion models for video generation. Project page: https://maxin-cn.github.io/latte_project.\nInference Latte-1 is now integrated into diffusers.\nYou can easily run Latte using the following code. We also support inference with 4/8-bit quantization, which can reduce GPU memory from 17 GB to 9 GB.\n#Please update the version of diffusers at leaset to 0.30.0 from diffusers import LattePipeline from diffusers.models import AutoencoderKLTemporalDecoder from torchvision.utils import save_image import torch import imageio torch.manual_seed(0)\ndevice = ‚Äúcuda‚Äù if torch.cuda.is_available() else ‚Äúcpu‚Äù video_length = 16 # 1 (text-to-image) or 16 (text-to-video) pipe = LattePipeline.from_pretrained(‚Äúmaxin-cn/Latte-1‚Äù, torch_dtype=torch.float16).to(device)\n#Using temporal decoder of VAE vae = AutoencoderKLTemporalDecoder.from_pretrained(‚Äúmaxin-cn/Latte-1‚Äù, subfolder=‚Äúvae_temporal_decoder‚Äù, torch_dtype=torch.float16).to(device) pipe.vae = vae\nprompt = ‚Äúa cat wearing sunglasses and working as a lifeguard at pool.‚Äù videos = pipe(prompt, video_length=video_length, output_type=‚Äòpt‚Äô).frames.cpu() ","date":1742688000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1742688000,"objectID":"d640771b41f4a924961048707a2a341b","permalink":"https://maxin-cn.github.io/post/latte-tmlr/","publishdate":"2025-03-23T00:00:00Z","relpermalink":"/post/latte-tmlr/","section":"post","summary":"Our paper Latte was finally accpeted by Transactions on Machine Learning Research (TMLR).","tags":["TMLR","Markdown"],"title":"üéâ one paper Latte was accpeted by Transactions on Machine Learning Research (TMLR)","type":"post"},{"authors":["Xin Ma"],"categories":null,"content":"one paper Cinemo was accpeted by IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\nAbstract Diffusion models have achieved significant progress in the task of image animation due to their powerful generative capabilities. However, preserving appearance consistency to the static input image, and avoiding abrupt motion change in the generated animation, remains challenging. In this paper, we introduce Cinemo, a novel image animation approach that aims at achieving better appearance consistency and motion smoothness. The core of Cinemo is to focus on learning the distribution of motion residuals, rather than directly predicting frames as in existing diffusion models. During the inference, we further mitigate the sudden motion changes in the generated video by introducing a novel DCT-based noise refinement strategy. To counteract the over-smoothing of motion, we introduce a dynamics degree control design for better control of the magnitude of motion. Altogether, these strategies enable Cinemo to produce highly consistent, smooth, and motion-controllable results. Extensive experiments compared with several state-of-the-art methods demonstrate the effectiveness and superiority of our proposed approach. In the end, we also demonstrate how our model can be applied for motion transfer or video editing of any given video. The project page is available at https://maxin-cn.github.io/cinemo_project/.\nSetup Download and set up the repo:\ngit clone https://github.com/maxin-cn/Cinemo cd Cinemo conda env create -f environment.yml conda activate cinemo Animation You can sample from our pre-trained Cinemo models. Weights for our pre-trained Cinemo model can be found here. The script has various arguments for adjusting sampling steps, changing the classifier-free guidance scale, etc:\nbash pipelines/animation.sh Related model weights will be downloaded automatically, and the following results can be found here.\n","date":1740614400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1740614400,"objectID":"03d8a1b9fa11a0904d5db063eec01d55","permalink":"https://maxin-cn.github.io/post/cinemo-cvpr/","publishdate":"2025-02-27T00:00:00Z","relpermalink":"/post/cinemo-cvpr/","section":"post","summary":"Our paper Cinemo was finally accpeted by IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) 2025.","tags":["CVPR","Markdown"],"title":"üéâ one paper Cinemo was accpeted by IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) 2025","type":"post"},{"authors":["Xin Ma"],"categories":null,"content":"one paper LaVie was accpeted by International Journal of Computer Vision (IJCV)\nAbstract This work aims to learn a high-quality text-to-video (T2V) generative model by leveraging a pre-trained text-to-image (T2I) model as a basis. It is a highly desirable yet challenging task to simultaneously (a) accomplish the synthesis of visually realistic and temporally coherent videos while (b) preserving the strong creative generation nature of the pre-trained T2I model. To this end, we propose LaVie, an integrated video generation framework that operates on cascaded video latent diffusion models, comprising a base T2V model, a temporal interpolation model, and a video super-resolution model. Our key insights are two-fold: (1) We reveal that the incorporation of simple temporal self-attentions, coupled with rotary positional encoding, adequately captures the temporal correlations inherent in video data. (2) Additionally, we validate that the process of joint image-video fine-tuning plays a pivotal role in producing high-quality and creative outcomes. To enhance the performance of LaVie, we contribute a comprehensive and diverse video dataset named Vimeo25M, consisting of 25 million text-video pairs that prioritize quality, diversity, and aesthetic appeal. Extensive experiments demonstrate that LaVie achieves state-of-the-art performance both quantitatively and qualitatively. Furthermore, we showcase the versatility of pre-trained LaVie models in various long video generation and personalized video synthesis applications. Project page: https://github.com/Vchitect/LaVie/.\n","date":1730073600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1730073600,"objectID":"0cace8e943cc12f7626dd6c1849a76e7","permalink":"https://maxin-cn.github.io/post/lavie-ijcv/","publishdate":"2024-10-28T00:00:00Z","relpermalink":"/post/lavie-ijcv/","section":"post","summary":"Our paper LaVie was finally accpeted by International Journal of Computer Vision (IJCV).","tags":["IJCV","Markdown"],"title":"üéâ one paper LaVie was accpeted by International Journal of Computer Vision (IJCV)","type":"post"},{"authors":["Xin Ma"],"categories":null,"content":"one paper LEO was accpeted by International Journal of Computer Vision (IJCV).\nAbstract Spatio-temporal coherency is a major challenge in synthesizing high quality videos, particularly in synthesizing human videos that contain rich global and local deformations. To resolve this challenge, previous approaches have resorted to different features in the generation process aimed at representing appearance and motion. However, in the absence of strict mechanisms to guarantee such disentanglement, a separation of motion from appearance has remained challenging, resulting in spatial distortions and temporal jittering that break the spatio-temporal coherency. Motivated by this, we here propose LEO, a novel framework for human video synthesis, placing emphasis on spatio-temporal coherency. Our key idea is to represent motion as a sequence of flow maps in the generation process, which inherently isolate motion from appearance. We implement this idea via a flow-based image animator and a Latent Motion Diffusion Model (LMDM). The former bridges a space of motion codes with the space of flow maps, and synthesizes video frames in a warp-and-inpaint manner. LMDM learns to capture motion prior in the training data by synthesizing sequences of motion codes. Extensive quantitative and qualitative analysis suggests that LEO significantly improves coherent synthesis of human videos over previous methods on the datasets TaichiHD, FaceForensics and CelebV-HQ. In addition, the effective disentanglement of appearance and motion in LEO allows for two additional tasks, namely infinite-length human video synthesis, as well as content-preserving video editing.\n","date":1724112000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1724112000,"objectID":"f4d508151a335c064143904515349f55","permalink":"https://maxin-cn.github.io/post/leo-ijcv/","publishdate":"2024-08-20T00:00:00Z","relpermalink":"/post/leo-ijcv/","section":"post","summary":"Our paper LEO was finally accpeted by International Journal of Computer Vision (IJCV).","tags":["IJCV","Markdown"],"title":"üéâ one paper LEO was accpeted by International Journal of Computer Vision (IJCV)","type":"post"},{"authors":["Xin Ma","Yaohui Wang","Gengyun Jia","Xinyuan Chen","Tien-Tsin Wong","Yuan-Fang Li","Cunjian Chen"],"categories":null,"content":" ","date":1721606400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721606400,"objectID":"5acfebc2b4d64639388a614a24d13e11","permalink":"https://maxin-cn.github.io/publication/conference-paper/cvpr_ma_2025/","publishdate":"2024-07-22T00:00:00Z","relpermalink":"/publication/conference-paper/cvpr_ma_2025/","section":"publication","summary":"Published in Computer Vision and Pattern Recognition (CVPR), 2025, Stars","tags":["CVPR"],"title":"Consistent and Controllable Image Animation with Motion Diffusion Models","type":"publication"},{"authors":null,"categories":null,"content":"","date":1718409600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1718409600,"objectID":"3a94d5b22596e839a64d8310d4069067","permalink":"https://maxin-cn.github.io/activity/","publishdate":"2024-06-15T00:00:00Z","relpermalink":"/activity/","section":"","summary":"","tags":null,"title":"Activity","type":"landing"},{"authors":null,"categories":null,"content":"","date":1718409600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1718409600,"objectID":"3c4864f00d23f7ea35511ec930ce1d9c","permalink":"https://maxin-cn.github.io/contact/","publishdate":"2024-06-15T00:00:00Z","relpermalink":"/contact/","section":"","summary":"","tags":null,"title":"Contact","type":"landing"},{"authors":null,"categories":null,"content":"","date":1718409600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1718409600,"objectID":"369971e71f1c506c9ca3ffab10099561","permalink":"https://maxin-cn.github.io/patent/","publishdate":"2024-06-15T00:00:00Z","relpermalink":"/patent/","section":"","summary":"","tags":null,"title":"Patent","type":"landing"},{"authors":null,"categories":null,"content":"","date":1718409600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1718409600,"objectID":"a7d933e1728d7cdba4b96b3eb7efd85e","permalink":"https://maxin-cn.github.io/selected_projects/","publishdate":"2024-06-15T00:00:00Z","relpermalink":"/selected_projects/","section":"","summary":"","tags":null,"title":"Project","type":"landing"},{"authors":null,"categories":null,"content":"","date":1718409600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1718409600,"objectID":"480c4de99851329b51acecc000e2e84f","permalink":"https://maxin-cn.github.io/publications/","publishdate":"2024-06-15T00:00:00Z","relpermalink":"/publications/","section":"","summary":"","tags":null,"title":"Publication","type":"landing"},{"authors":null,"categories":null,"content":"","date":1718409600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1718409600,"objectID":"689ffd5b24716d4d2e9da3c7427a2867","permalink":"https://maxin-cn.github.io/talk/","publishdate":"2024-06-15T00:00:00Z","relpermalink":"/talk/","section":"","summary":"","tags":null,"title":"Talks","type":"landing"},{"authors":[],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature. ","date":1718359200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1718359200,"objectID":"4e352c7c13f6ee178d031b4ea0c57fef","permalink":"https://maxin-cn.github.io/talk/application-and-expansion-of-dit-architecture-in-video-generation-models/","publishdate":"2023-06-14T14:00:00Z","relpermalink":"/talk/application-and-expansion-of-dit-architecture-in-video-generation-models/","section":"event","summary":"A talk introducing Latte.","tags":[],"title":"Application and expansion of DiT architecture in video generation models","type":"event"},{"authors":["Xin Ma"],"categories":null,"content":"I have been invited to give a talk on ‚ÄúApplication and Expansion of the DiT Architecture in Video Generation Models‚Äù at Intelligent Things.\nAbstract In this talk, I will share the current state and recent advances in video generation research. I will then introduce Latte, a Transformer-based video diffusion model. Following that, I will present some visual comparisons of generated videos. Finally, I will conclude with a discussion on potential future directions and task extensions in text-to-video generation.\n","date":1718323200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1718323200,"objectID":"c08870a5325e11d33852d7c8d5d0621d","permalink":"https://maxin-cn.github.io/post/invited-talk-latte/","publishdate":"2024-06-14T00:00:00Z","relpermalink":"/post/invited-talk-latte/","section":"post","summary":"I have been invited to give a talk on \"Application and Expansion of the DiT Architecture in Video Generation Models\" at Intelligent Things.","tags":["Intelligent things","Markdown"],"title":"üë©üèº‚Äçüè´ Invited talk at Intelligent things","type":"post"},{"authors":["Xin Ma"],"categories":null,"content":"We release the codes of Cinemo on Github.\nSetup Download and set up the repo:\ngit clone https://github.com/maxin-cn/Cinemo cd Cinemo conda env create -f environment.yml conda activate cinemo Animation You can sample from our pre-trained Cinemo models. Weights for our pre-trained Cinemo model can be found here. The script has various arguments for adjusting sampling steps, changing the classifier-free guidance scale, etc:\nbash pipelines/animation.sh Related model weights will be downloaded automatically, and the following results can be found here.\n","date":1717286400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1717286400,"objectID":"030c20f56405d7ab3bf71a72213d72e5","permalink":"https://maxin-cn.github.io/post/cinemo-code/","publishdate":"2024-06-02T00:00:00Z","relpermalink":"/post/cinemo-code/","section":"post","summary":"We release the Cinemo codes on Github.","tags":["Github","Markdown"],"title":"‚úÖ Release the codes of Cinemo","type":"post"},{"authors":null,"categories":null,"content":"","date":1716595200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1716595200,"objectID":"f76fd61ffc133f7669794a93836b7c5d","permalink":"https://maxin-cn.github.io/project/cinemo/","publishdate":"2024-05-25T00:00:00Z","relpermalink":"/project/cinemo/","section":"project","summary":"This framework offers simpler, more precise user control and better image animation performance.","tags":["Image Animation","Diffusion"],"title":"Cinemo - An image animation Framework (CVPR 2025)","type":"project"},{"authors":["Xin Ma"],"categories":null,"content":" We release the chckpoints of Latte-1 on Hugging Face.\nInference Latte-1 is now integrated into diffusers.\nYou can easily run Latte using the following code. We also support inference with 4/8-bit quantization, which can reduce GPU memory from 17 GB to 9 GB.\n#Please update the version of diffusers at leaset to 0.30.0 from diffusers import LattePipeline from diffusers.models import AutoencoderKLTemporalDecoder from torchvision.utils import save_image import torch import imageio torch.manual_seed(0)\ndevice = ‚Äúcuda‚Äù if torch.cuda.is_available() else ‚Äúcpu‚Äù video_length = 16 # 1 (text-to-image) or 16 (text-to-video) pipe = LattePipeline.from_pretrained(‚Äúmaxin-cn/Latte-1‚Äù, torch_dtype=torch.float16).to(device)\n#Using temporal decoder of VAE vae = AutoencoderKLTemporalDecoder.from_pretrained(‚Äúmaxin-cn/Latte-1‚Äù, subfolder=‚Äúvae_temporal_decoder‚Äù, torch_dtype=torch.float16).to(device) pipe.vae = vae\nprompt = ‚Äúa cat wearing sunglasses and working as a lifeguard at pool.‚Äù videos = pipe(prompt, video_length=video_length, output_type=‚Äòpt‚Äô).frames.cpu() ","date":1716422400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1716422400,"objectID":"cb8397859577b8634f38fbc63f04eb37","permalink":"https://maxin-cn.github.io/post/latte-code/","publishdate":"2024-05-23T00:00:00Z","relpermalink":"/post/latte-code/","section":"post","summary":"We release the chckpoints of Latte-1 on Hugging Face.","tags":["Hugging Face","Markdown"],"title":"‚úÖ Release the checkpoints of Latte-1","type":"post"},{"authors":["Yi Wang","Yinan He","Yizhuo Li","Kunchang Li","Jiashuo Yu","Xin Ma","Xinhao Li","Guo Chen","Xinyuan Chen","Yaohui Wang","Conghui He","Ping Luo","Ziwei Liu","Yali Wang","Limin Wang","Yu Qiao"],"categories":null,"content":" ","date":1705366500,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1705366500,"objectID":"82d4c8f3405a0c72c4926a4e1febf010","permalink":"https://maxin-cn.github.io/publication/conference-paper/iclr_wang_2024/","publishdate":"2024-01-16T00:55:00Z","relpermalink":"/publication/conference-paper/iclr_wang_2024/","section":"publication","summary":"Published in International Conference on Learning Representations (ICLR), 2024, Stars","tags":["ICLR"],"title":"InternVid: A Large-scale Video-Text Dataset for Multimodal Understanding and Generation","type":"publication"},{"authors":["Xinyuan Chen","Yaohui Wang","Lingjun Zhang","Shaobin Zhuang","Xin Ma","Jiashuo Yu","Yali Wang","Dahua Lin","Yu Qiao","Ziwei Liu"],"categories":null,"content":" ","date":1705365360,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1705365360,"objectID":"023b44e626932117d58f0e7054b2b548","permalink":"https://maxin-cn.github.io/publication/conference-paper/iclr_chen_2024/","publishdate":"2024-01-16T00:36:00Z","relpermalink":"/publication/conference-paper/iclr_chen_2024/","section":"publication","summary":"Published in International Conference on Learning Representations (ICLR), 2024, Stars","tags":["ICLR"],"title":"SEINE: Short-to-Long Video Diffusion Model for Generative Transition and Prediction","type":"publication"},{"authors":["Xin Ma","Yaohui Wang","Xinyuan Chen","Gengyun Jia","Ziwei Liu","Yuan-Fang Li","Cunjian Chen","Yu Qiao"],"categories":null,"content":" ","date":1704412800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1704412800,"objectID":"8b4436d96088bb97d1bb95fa23376cd6","permalink":"https://maxin-cn.github.io/publication/journal-article/tmlr_latte_2025/","publishdate":"2024-01-05T00:00:00Z","relpermalink":"/publication/journal-article/tmlr_latte_2025/","section":"publication","summary":"Published in Transactions on Machine Learning Research (TMLR), 2025, Stars","tags":["TMLR"],"title":"Latte: Latent Diffusion Transformer for Video Generation","type":"publication"},{"authors":null,"categories":null,"content":"","date":1698105600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1698105600,"objectID":"57b84d5054dbd8444a1ff4918ce4c623","permalink":"https://maxin-cn.github.io/work_experience/","publishdate":"2023-10-24T00:00:00Z","relpermalink":"/work_experience/","section":"","summary":"","tags":null,"title":"Experience","type":"landing"},{"authors":null,"categories":null,"content":"","date":1695859200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1695859200,"objectID":"71989c222514edbdb50fd42362ea0372","permalink":"https://maxin-cn.github.io/project/latte/","publishdate":"2023-09-28T00:00:00Z","relpermalink":"/project/latte/","section":"project","summary":"A simple and general latent video diffusion model incorporating sptio-temporal Transformers for video generation.","tags":["Text-to-video Generation","Transformers"],"title":"Latte - A first open source Transformer-based Video Diffusion Generation Framework (TMLR 2025)","type":"project"},{"authors":null,"categories":null,"content":"","date":1695772800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1695772800,"objectID":"e95e3430044ac4a4f6735152222621f0","permalink":"https://maxin-cn.github.io/project/lavie/","publishdate":"2023-09-27T00:00:00Z","relpermalink":"/project/lavie/","section":"project","summary":"A large-scale text-to-video framework that produces high-quality and temporally coherent videos. This framework operates on cascaded video latent diffusion models, comprising a base T2V model, a temporal interpolation model, and a video super-resolution model.","tags":["Text-to-video Generation"],"title":"LaVie - A High-Quality Video Generation Framework (IJCV 2024)","type":"project"},{"authors":["Xin Ma","Yaohui Wang","Xinyuan Chen","Xin Ma","Shangchen Zhou","Ziqi Huang","Yi Wang","Ceyuan Yang","Yinan He","Jiashuo Yu","Peiqing Yang","Yuwei Guo","Tianxing Wu","Chenyang Si","Yuming Jiang","Cunjian Chen","Chen Change Loy","Bo Dai","Dahua Lin","Yu Qiao","Ziwei Liu"],"categories":null,"content":" ","date":1695686400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1695686400,"objectID":"8bf79053b14b853764d1fe5628f84773","permalink":"https://maxin-cn.github.io/publication/journal-article/ijcv_lavie_2024/","publishdate":"2023-09-26T00:00:00Z","relpermalink":"/publication/journal-article/ijcv_lavie_2024/","section":"publication","summary":"Published in International Journal of Computer Vision (IJCV), 2024, Stars","tags":["IJCV"],"title":"LaVie: High-Quality Video Generation with Cascaded Latent Diffusion Models","type":"publication"},{"authors":["Xin Ma","Xiaoqiang Zhou","Huaibo Huang","Gengyun Jia","Yaohui Wang","Xinyuan Chen","Cunjian Chen"],"categories":null,"content":"\r","date":1691539200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1691539200,"objectID":"7ee18d635976c67d5dda344ba3b71914","permalink":"https://maxin-cn.github.io/publication/journal-article/eswa_2023/","publishdate":"2023-08-09T00:00:00Z","relpermalink":"/publication/journal-article/eswa_2023/","section":"publication","summary":"Published in Expert Systems with Applications (ESWA), 2023, Stars","tags":["ESWA"],"title":"Uncertainty-Aware Image Inpainting with Adaptive Feedback Network","type":"publication"},{"authors":["Yaohui Wang","Xin Ma","Xinyuan Chen","Cunjian Chen","Antitza Dantcheva","Bo Dai","Yu Qiao"],"categories":null,"content":" ","date":1683331200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1683331200,"objectID":"75000dd0a22d1379ccc22062b73e916c","permalink":"https://maxin-cn.github.io/publication/journal-article/ijcv_leo_2024/","publishdate":"2024-08-20T00:00:00Z","relpermalink":"/publication/journal-article/ijcv_leo_2024/","section":"publication","summary":"Published in International Journal of Computer Vision (IJCV), 2024","tags":["IJCV"],"title":"LEO: Generative Latent Image Animator for Human Video Synthesis","type":"publication"},{"authors":["Mandi Luo","Xin Ma","Huaibo Huang","Ran He"],"categories":null,"content":"\r","date":1656633600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1656633600,"objectID":"7fc29e754d4db4dcd5a9d45c1a1606e2","permalink":"https://maxin-cn.github.io/publication/conference-paper/prcv_2022/","publishdate":"2022-07-01T00:00:00Z","relpermalink":"/publication/conference-paper/prcv_2022/","section":"publication","summary":"Published in Pattern Recognition and Computer Vision (PRCV), 2022","tags":["PRCV"],"title":"Style-Based Attentive Network for Real-World Face Hallucination","type":"publication"},{"authors":null,"categories":null,"content":"","date":1655251200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1655251200,"objectID":"fd2b179a695644f4041323bf1f73740f","permalink":"https://maxin-cn.github.io/bak_index/","publishdate":"2022-06-15T00:00:00Z","relpermalink":"/bak_index/","section":"","summary":"","tags":null,"title":"","type":"landing"},{"authors":["Huanyu Wang","Junjie Liu","Xin Ma","Yang Yong","Zhenhua Chai","Jianxin Wu"],"categories":null,"content":"\r","date":1651363200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1651363200,"objectID":"c06e5c616a07a4b9cc8492b547f093a8","permalink":"https://maxin-cn.github.io/publication/conference-paper/cvpr_2022/","publishdate":"2022-05-01T00:00:00Z","relpermalink":"/publication/conference-paper/cvpr_2022/","section":"publication","summary":"Published in Computer Vision and Pattern Recognition (CVPR), 2022,","tags":["CVPR"],"title":"Compressing Models with Few Samples: Mimicking then Replacing","type":"publication"},{"authors":["Xin Ma","Xiaoqiang Zhou","Huaibo Huang","Gengyun Jia","Zhenhua Chai","Xiaolin Wei"],"categories":null,"content":"\r","date":1637971200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1637971200,"objectID":"ef966bb40071611b8a61dbda2f92d6cd","permalink":"https://maxin-cn.github.io/publication/journal-article/pr_2022/","publishdate":"2021-11-27T00:00:00Z","relpermalink":"/publication/journal-article/pr_2022/","section":"publication","summary":"Published in Pattern Recognition (PR), 2022","tags":["PR"],"title":"Contrastive attention network with dense field estimation for face completion","type":"publication"},{"authors":["Mandi Luo","Xin Ma","Zhihang Li","Jie Cao","Ran He"],"categories":null,"content":"\r","date":1635292800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635292800,"objectID":"318acef0f1616461feed18b6f5d10f77","permalink":"https://maxin-cn.github.io/publication/journal-article/tifs_partial_2021/","publishdate":"2021-10-27T00:00:00Z","relpermalink":"/publication/journal-article/tifs_partial_2021/","section":"publication","summary":"Published in IEEE Transactions on Information Forensics and Security (T-IFS), 2021","tags":["TIFS"],"title":"Partial NIR-VIS heterogeneous face recognition with automatic saliency search","type":"publication"},{"authors":["Xin Ma","Xiaoqiang Zhou","Huaibo Huang","Zhenhua Chai","Xiaolin Wei","Ran He"],"categories":null,"content":"\r","date":1633046400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1633046400,"objectID":"ea8eaf492f9c0c0a56b15e6685f11f4c","permalink":"https://maxin-cn.github.io/publication/conference-paper/icpr_ma_2021/","publishdate":"2021-10-01T00:00:00Z","relpermalink":"/publication/conference-paper/icpr_ma_2021/","section":"publication","summary":"Published in International Conference on Pattern Recognition (ICPR), 2021","tags":["ICPR"],"title":"Free-form image inpainting via contrastive attention network","type":"publication"},{"authors":["Yuhe Ding","Xin Ma","Mandi Luo","Aihua Zheng","Ran He"],"categories":null,"content":"\r","date":1633046400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1633046400,"objectID":"4d61fb7539c1a19617e81930ed6f45fa","permalink":"https://maxin-cn.github.io/publication/conference-paper/icpr_ding_2021/","publishdate":"2021-10-01T00:00:00Z","relpermalink":"/publication/conference-paper/icpr_ding_2021/","section":"publication","summary":"Published in International Conference on Pattern Recognition (ICPR), 2021","tags":["ICPR"],"title":"Unsupervised Contrastive Photo-to-Caricature Translation based on Auto-distortion","type":"publication"},{"authors":["Gengyun Jia","Meisong Zheng","Chuanrui Hu","Xin Ma","Yuting Xu","Luoqi Liu","Yafeng Deng","Ran He"],"categories":null,"content":"\r","date":1622505600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1622505600,"objectID":"bc3d6a801101e01faed22698ee7479a4","permalink":"https://maxin-cn.github.io/publication/journal-article/tbiom_2021/","publishdate":"2021-06-01T00:00:00Z","relpermalink":"/publication/journal-article/tbiom_2021/","section":"publication","summary":"Published in IEEE Transactions on Biometrics, Behavior, and Identity Science (T-BIOM), 2021","tags":["TBIOM"],"title":"Inconsistency-aware wavelet dual-branch network for face forgery detection","type":"publication"},{"authors":["Mandi Luo","Jie Cao","Xin Ma","Xiaoyu Zhang","Ran He"],"categories":null,"content":"\r","date":1611705600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1611705600,"objectID":"a2542ed346f0643f1a47e707f1f0512d","permalink":"https://maxin-cn.github.io/publication/journal-article/tifs_fagan_2021/","publishdate":"2021-01-27T00:00:00Z","relpermalink":"/publication/journal-article/tifs_fagan_2021/","section":"publication","summary":"Published in IEEE Transactions on Information Forensics and Security (T-IFS), 2021","tags":["TIFS"],"title":"FA-GAN: face augmentation GAN for deformation-invariant face recognition","type":"publication"}]