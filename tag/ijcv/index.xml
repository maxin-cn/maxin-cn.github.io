<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>IJCV | Xin Ma</title>
    <link>https://maxin-cn.github.io/tag/ijcv/</link>
      <atom:link href="https://maxin-cn.github.io/tag/ijcv/index.xml" rel="self" type="application/rss+xml" />
    <description>IJCV</description>
    <generator>Hugo Blox Builder (https://hugoblox.com)</generator><language>en-us</language><lastBuildDate>Mon, 28 Oct 2024 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://maxin-cn.github.io/media/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_3.png</url>
      <title>IJCV</title>
      <link>https://maxin-cn.github.io/tag/ijcv/</link>
    </image>
    
    <item>
      <title>ðŸŽ‰ one paper LaVie was accpeted by International Journal of Computer Vision (IJCV)</title>
      <link>https://maxin-cn.github.io/post/lavie-ijcv/</link>
      <pubDate>Mon, 28 Oct 2024 00:00:00 +0000</pubDate>
      <guid>https://maxin-cn.github.io/post/lavie-ijcv/</guid>
      <description>&lt;!-- 

&lt;details class=&#34;toc-inpage d-print-none  &#34; open&gt;
  &lt;summary class=&#34;font-weight-bold&#34;&gt;Table of Contents&lt;/summary&gt;
  &lt;nav id=&#34;TableOfContents&#34;&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;a href=&#34;#abstract&#34;&gt;Abstract&lt;/a&gt;&lt;/li&gt;
  &lt;/ul&gt;
&lt;/nav&gt;
&lt;/details&gt;
 --&gt;
&lt;p&gt;one paper &lt;a href=&#34;../../publication/journal-article/ijcv_lavie_2024/&#34;&gt;LaVie&lt;/a&gt; was accpeted by International Journal of Computer Vision (IJCV)&lt;/p&gt;
&lt;h2 id=&#34;abstract&#34;&gt;Abstract&lt;/h2&gt;
&lt;p&gt;This work aims to learn a high-quality text-to-video (T2V) generative model by leveraging a pre-trained text-to-image (T2I) model as a basis. It is a highly desirable yet challenging task to simultaneously (a) accomplish the synthesis of visually realistic and temporally coherent videos while (b) preserving the strong creative generation nature of the pre-trained T2I model. To this end, we propose LaVie, an integrated video generation framework that operates on cascaded video latent diffusion models, comprising a base T2V model, a temporal interpolation model, and a video super-resolution model. Our key insights are two-fold: (1) We reveal that the incorporation of simple temporal self-attentions, coupled with rotary positional encoding, adequately captures the temporal correlations inherent in video data. (2) Additionally, we validate that the process of joint image-video fine-tuning plays a pivotal role in producing high-quality and creative outcomes. To enhance the performance of LaVie, we contribute a comprehensive and diverse video dataset named Vimeo25M, consisting of 25 million text-video pairs that prioritize quality, diversity, and aesthetic appeal. Extensive experiments demonstrate that LaVie achieves state-of-the-art performance both quantitatively and qualitatively. Furthermore, we showcase the versatility of pre-trained LaVie models in various long video generation and personalized video synthesis applications. Project page: &lt;a href=&#34;https://github.com/Vchitect/LaVie/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/Vchitect/LaVie/&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>ðŸŽ‰ one paper LEO was accpeted by International Journal of Computer Vision (IJCV)</title>
      <link>https://maxin-cn.github.io/post/leo-ijcv/</link>
      <pubDate>Tue, 20 Aug 2024 00:00:00 +0000</pubDate>
      <guid>https://maxin-cn.github.io/post/leo-ijcv/</guid>
      <description>&lt;p&gt;one paper &lt;a href=&#34;../../publication/journal-article/ijcv_leo_2024/&#34;&gt;LEO&lt;/a&gt; was accpeted by International Journal of Computer Vision (IJCV).&lt;/p&gt;
&lt;h2 id=&#34;abstract&#34;&gt;Abstract&lt;/h2&gt;
&lt;p&gt;Spatio-temporal coherency is a major challenge in synthesizing high quality videos, particularly in synthesizing human videos that contain rich global and local deformations. To resolve this challenge, previous approaches have resorted to different features in the generation process aimed at representing appearance and motion. However, in the absence of strict mechanisms to guarantee such disentanglement, a separation of motion from appearance has remained challenging, resulting in spatial distortions and temporal jittering that break the spatio-temporal coherency. Motivated by this, we here propose LEO, a novel framework for human video synthesis, placing emphasis on spatio-temporal coherency. Our key idea is to represent motion as a sequence of flow maps in the generation process, which inherently isolate motion from appearance. We implement this idea via a flow-based image animator and a Latent Motion Diffusion Model (LMDM). The former bridges a space of motion codes with the space of flow maps, and synthesizes video frames in a warp-and-inpaint manner. LMDM learns to capture motion prior in the training data by synthesizing sequences of motion codes. Extensive quantitative and qualitative analysis suggests that LEO significantly improves coherent synthesis of human videos over previous methods on the datasets TaichiHD, FaceForensics and CelebV-HQ. In addition, the effective disentanglement of appearance and motion in LEO allows for two additional tasks, namely infinite-length human video synthesis, as well as content-preserving video editing.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>LaVie: High-Quality Video Generation with Cascaded Latent Diffusion Models</title>
      <link>https://maxin-cn.github.io/publication/journal-article/ijcv_lavie_2024/</link>
      <pubDate>Tue, 26 Sep 2023 00:00:00 +0000</pubDate>
      <guid>https://maxin-cn.github.io/publication/journal-article/ijcv_lavie_2024/</guid>
      <description>&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;
&lt;!-- 

&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Create your slides in Markdown - click the &lt;em&gt;Slides&lt;/em&gt; button to check out the example.
  &lt;/div&gt;
&lt;/div&gt;


Add the publication&#39;s **full text** or **supplementary notes** here. You can use rich formatting such as including [code, math, and images](https://docs.hugoblox.com/content/writing-markdown-latex/). --&gt;</description>
    </item>
    
    <item>
      <title>LEO: Generative Latent Image Animator for Human Video Synthesis</title>
      <link>https://maxin-cn.github.io/publication/journal-article/ijcv_leo_2024/</link>
      <pubDate>Sat, 06 May 2023 00:00:00 +0000</pubDate>
      <guid>https://maxin-cn.github.io/publication/journal-article/ijcv_leo_2024/</guid>
      <description>&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;
&lt;!-- 

&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Create your slides in Markdown - click the &lt;em&gt;Slides&lt;/em&gt; button to check out the example.
  &lt;/div&gt;
&lt;/div&gt;


Add the publication&#39;s **full text** or **supplementary notes** here. You can use rich formatting such as including [code, math, and images](https://docs.hugoblox.com/content/writing-markdown-latex/). --&gt;</description>
    </item>
    
  </channel>
</rss>
