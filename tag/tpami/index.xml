<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>TPAMI | Xin Ma</title>
    <link>https://maxin-cn.github.io/tag/tpami/</link>
      <atom:link href="https://maxin-cn.github.io/tag/tpami/index.xml" rel="self" type="application/rss+xml" />
    <description>TPAMI</description>
    <generator>Hugo Blox Builder (https://hugoblox.com)</generator><language>en-us</language><lastBuildDate>Sun, 08 Feb 2026 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://maxin-cn.github.io/media/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_3.png</url>
      <title>TPAMI</title>
      <link>https://maxin-cn.github.io/tag/tpami/</link>
    </image>
    
    <item>
      <title>ðŸŽ‰ Our paper MiraMo was accpeted by IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
      <link>https://maxin-cn.github.io/post/miramo-tpami/</link>
      <pubDate>Sun, 08 Feb 2026 00:00:00 +0000</pubDate>
      <guid>https://maxin-cn.github.io/post/miramo-tpami/</guid>
      <description>&lt;p&gt;one paper &lt;a href=&#34;../../publication/journal-article/miramo_tpami_2026/&#34;&gt;MiraMo&lt;/a&gt; was accpeted by IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)&lt;/p&gt;
&lt;h2 id=&#34;abstract&#34;&gt;Abstract&lt;/h2&gt;
&lt;p&gt;Image animation has seen significant progress, driven by the powerful generative capabilities of diffusion models.
However, maintaining appearance consistency with static input images and mitigating abrupt motion transitions in generated animations remain persistent challenges. While text-to-video (T2V) generation has demonstrated impressive performance with diffusion transformer models, the image animation field still largely relies on U-Net-based diffusion models, which lag behind the latest T2V approaches. Moreover, the quadratic complexity of vanilla self-attention mechanisms in Transformers imposes heavy computational demands, making image animation particularly resource-intensive.
To address these issues, we propose MiraMo, a framework designed to enhance efficiency, appearance consistency, and motion smoothness in image animation.
Specifically, MiraMo introduces three key elements: (1) A foundational text-to-video architecture replacing vanilla self-attention with efficient linear attention to reduce computational overhead while preserving generation quality; (2) A novel motion residual learning paradigm that focuses on modeling motion dynamics rather than directly predicting frames, improving temporal consistency; and (3) A DCT-based noise refinement strategy during inference to suppress sudden motion artifacts, complemented by a dynamics control module to balance motion smoothness and expressiveness.
Extensive experiments against state-of-the-art methods validate the superiority of MiraMo in generating consistent, smooth, and controllable animations with accelerated inference speed. Additionally, we demonstrate the versatility of MiraMo through applications in motion transfer and video editing tasks. The project page is available at: &lt;a href=&#34;https://maxin-cn.github.io/miramo_project&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://maxin-cn.github.io/miramo_project&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Consistent and Controllable Image Animation with Linear Motion Diffusion Transformers</title>
      <link>https://maxin-cn.github.io/publication/journal-article/miramo_tpami_2026/</link>
      <pubDate>Sun, 10 Aug 2025 00:00:00 +0000</pubDate>
      <guid>https://maxin-cn.github.io/publication/journal-article/miramo_tpami_2026/</guid>
      <description>&lt;!-- 

&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;
 --&gt;
&lt;!-- 

&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Create your slides in Markdown - click the &lt;em&gt;Slides&lt;/em&gt; button to check out the example.
  &lt;/div&gt;
&lt;/div&gt;
 --&gt;
&lt;!-- Supplementary notes can be added here, including [code, math, and images](https://wowchemy.com/docs/writing-markdown-latex/). --&gt;
</description>
    </item>
    
  </channel>
</rss>
