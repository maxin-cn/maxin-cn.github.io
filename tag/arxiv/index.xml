<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>arXiv | Xin Ma</title>
    <link>https://maxin-cn.github.io/tag/arxiv/</link>
      <atom:link href="https://maxin-cn.github.io/tag/arxiv/index.xml" rel="self" type="application/rss+xml" />
    <description>arXiv</description>
    <generator>Hugo Blox Builder (https://hugoblox.com)</generator><language>en-us</language><lastBuildDate>Sun, 10 Aug 2025 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://maxin-cn.github.io/media/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_3.png</url>
      <title>arXiv</title>
      <link>https://maxin-cn.github.io/tag/arxiv/</link>
    </image>
    
    <item>
      <title>⬆️ one paper Miramo exploring image animation using linear attention was released on arXiv</title>
      <link>https://maxin-cn.github.io/post/miramo-paper/</link>
      <pubDate>Sun, 10 Aug 2025 00:00:00 +0000</pubDate>
      <guid>https://maxin-cn.github.io/post/miramo-paper/</guid>
      <description>&lt;!-- Our paper [MiraMo](../../publication/preprint/miramo/) was released on [arxiv](https://arxiv.org/abs/2508.07246). --&gt;
&lt;p&gt;We have released &lt;a href=&#34;../../publication/preprint/miramo/&#34;&gt;MiraMo&lt;/a&gt; on arXiv. It is among the first papers to explore linear attention for video generation.&lt;/p&gt;
&lt;h2 id=&#34;setup&#34;&gt;Setup&lt;/h2&gt;
&lt;p&gt;Download and set up the repo:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;
&lt;pre class=&#34;chroma&#34;&gt;
&lt;code&gt;
git clone https://github.com/maxin-cn/MiraMo
cd MiraMo
conda env create -f environment.yml
conda activate miramo
&lt;/code&gt;
&lt;/pre&gt;
&lt;/div&gt;
&lt;h2 id=&#34;animation&#34;&gt;Animation&lt;/h2&gt;
&lt;p&gt;You can sample from our pre-trained MiraMo models. Weights for our pre-trained MiraMo model can be found &lt;a href=&#34;https://huggingface.co/maxin-cn/MiraMo/tree/main&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;. The script has various arguments for adjusting sampling steps, changing the classifier-free guidance scale, etc:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;
&lt;pre class=&#34;chroma&#34;&gt;
&lt;code&gt;
bash pipelines/animation.sh
&lt;/code&gt;
&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;Related model weights will be downloaded automatically, and the following results can be found &lt;a href=&#34;https://github.com/maxin-cn/MiraMo/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>⬆️ one paper OmniPainter for the sylized T2I generation was released on arXiv</title>
      <link>https://maxin-cn.github.io/post/omnipainter-code/</link>
      <pubDate>Tue, 27 May 2025 00:00:00 +0000</pubDate>
      <guid>https://maxin-cn.github.io/post/omnipainter-code/</guid>
      <description>&lt;p&gt;Our paper &lt;a href=&#34;../../publication/preprint/omnipainter/&#34;&gt;OmniPainter&lt;/a&gt; for the sylized text-to-image generation was released on &lt;a href=&#34;https://arxiv.org/abs/2505.19063&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;arxiv&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;setup&#34;&gt;Setup&lt;/h2&gt;
&lt;p&gt;Download and set up the repo:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;
&lt;pre class=&#34;chroma&#34;&gt;
&lt;code&gt;
git clone https://github.com/maxin-cn/OmniPainter
cd OmniPainter
conda env create -f environment.yml
conda activate omnipainter
&lt;/code&gt;
&lt;/pre&gt;
&lt;/div&gt;
&lt;h2 id=&#34;stylized-image-generation&#34;&gt;Stylized image generation&lt;/h2&gt;
&lt;p&gt;You can sample high-quality images that match both the given prompt and the style reference image within just 4 to 6 timesteps, without requiring any inversion. The script has various arguments for adjusting sampling steps, changing the classifier-free guidance scale, etc:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;
&lt;pre class=&#34;chroma&#34;&gt;
&lt;code&gt;
bash run.sh
&lt;/code&gt;
&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;Related model weights will be downloaded automatically and following results can be obtained &lt;a href=&#34;https://github.com/maxin-cn/OmniPainter/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Training-free Stylized Text-to-Image Generation with Fast Inference</title>
      <link>https://maxin-cn.github.io/publication/preprint/omnipainter/</link>
      <pubDate>Tue, 27 May 2025 00:00:00 +0000</pubDate>
      <guid>https://maxin-cn.github.io/publication/preprint/omnipainter/</guid>
      <description>&lt;!-- 

&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;
 --&gt;
&lt;!-- 

&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Create your slides in Markdown - click the &lt;em&gt;Slides&lt;/em&gt; button to check out the example.
  &lt;/div&gt;
&lt;/div&gt;
 --&gt;
&lt;!-- Supplementary notes can be added here, including [code, math, and images](https://wowchemy.com/docs/writing-markdown-latex/). --&gt;
</description>
    </item>
    
  </channel>
</rss>
