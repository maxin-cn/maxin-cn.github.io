<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Markdown | Xin Ma</title>
    <link>https://maxin-cn.github.io/tag/markdown/</link>
      <atom:link href="https://maxin-cn.github.io/tag/markdown/index.xml" rel="self" type="application/rss+xml" />
    <description>Markdown</description>
    <generator>Hugo Blox Builder (https://hugoblox.com)</generator><language>en-us</language><lastBuildDate>Fri, 13 Jun 2025 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://maxin-cn.github.io/media/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_3.png</url>
      <title>Markdown</title>
      <link>https://maxin-cn.github.io/tag/markdown/</link>
    </image>
    
    <item>
      <title>üéâ I was recognized as one of the Outstanding Reviewers for CVPR 2025</title>
      <link>https://maxin-cn.github.io/post/outstanding-reviewer-cvpr2025/</link>
      <pubDate>Fri, 13 Jun 2025 00:00:00 +0000</pubDate>
      <guid>https://maxin-cn.github.io/post/outstanding-reviewer-cvpr2025/</guid>
      <description>&lt;p&gt;I am glad to have been selected as one of the Outstanding Reviewers by the CVPR 2025 committee.&lt;/p&gt;
&lt;h2 id=&#34;details-of-the-outstanding-reviewers-for-cvpr-2025&#34;&gt;Details of the Outstanding Reviewers for CVPR 2025&lt;/h2&gt;
&lt;p&gt;This recognition reflects not only the quality and depth of the reviews but also the commitment to fairness, clarity, and timely feedback throughout the review process. A total of 12,593 reviewers submitted reviews for CVPR 2025. A total of 711 outstanding reviewers are chosen from reviewers who have submitted at least 3 reviews and are highly rated by area chairs. Total list of outstanding reviewers is available &lt;a href=&#34;https://cvpr.thecvf.com/Conferences/2025/ProgramCommittee&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>üíûÔ∏è Always pursuse research collaborations. Feel free to contact me if you are interested</title>
      <link>https://maxin-cn.github.io/post/research-collaboration/</link>
      <pubDate>Mon, 02 Jun 2025 00:00:00 +0000</pubDate>
      <guid>https://maxin-cn.github.io/post/research-collaboration/</guid>
      <description>&lt;p&gt;I am always pursuse research collaborations. Feel free to contact me if you are interested.&lt;/p&gt;
&lt;h2 id=&#34;my-research-interest&#34;&gt;My Research Interest&lt;/h2&gt;
&lt;p&gt;I am focus on image super-resolution and inpainting, model compression, face recognition, video generation, large-scale generative models and so on.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>‚¨ÜÔ∏è one paper OmniPainter was released on arxiv</title>
      <link>https://maxin-cn.github.io/post/omnipainter-code/</link>
      <pubDate>Tue, 27 May 2025 00:00:00 +0000</pubDate>
      <guid>https://maxin-cn.github.io/post/omnipainter-code/</guid>
      <description>&lt;p&gt;Our paper &lt;a href=&#34;../../publication/preprint/omnipainter/&#34;&gt;OmniPainter&lt;/a&gt; was released on &lt;a href=&#34;https://arxiv.org/abs/2505.19063&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;arxiv&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;setup&#34;&gt;Setup&lt;/h2&gt;
&lt;p&gt;Download and set up the repo:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;
&lt;pre class=&#34;chroma&#34;&gt;
&lt;code&gt;
git clone https://github.com/maxin-cn/OmniPainter
cd OmniPainter
conda env create -f environment.yml
conda activate omnipainter
&lt;/code&gt;
&lt;/pre&gt;
&lt;/div&gt;
&lt;h2 id=&#34;stylized-image-generation&#34;&gt;Stylized image generation&lt;/h2&gt;
&lt;p&gt;You can sample high-quality images that match both the given prompt and the style reference image within just 4 to 6 timesteps, without requiring any inversion. The script has various arguments for adjusting sampling steps, changing the classifier-free guidance scale, etc:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;
&lt;pre class=&#34;chroma&#34;&gt;
&lt;code&gt;
bash run.sh
&lt;/code&gt;
&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;Related model weights will be downloaded automatically and following results can be obtained &lt;a href=&#34;https://github.com/maxin-cn/OmniPainter/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>üéâ one paper Latte was accpeted by Transactions on Machine Learning Research (TMLR)</title>
      <link>https://maxin-cn.github.io/post/latte-tmlr/</link>
      <pubDate>Sun, 23 Mar 2025 00:00:00 +0000</pubDate>
      <guid>https://maxin-cn.github.io/post/latte-tmlr/</guid>
      <description>&lt;p&gt;Our paper &lt;a href=&#34;../../publication/journal-article/tmlr_latte_2025/&#34;&gt;Latte&lt;/a&gt; was accpeted by Transactions on Machine Learning Research (TMLR)&lt;/p&gt;
&lt;h2 id=&#34;abstract&#34;&gt;Abstract&lt;/h2&gt;
&lt;p&gt;We propose Latte, a novel Latent Diffusion Transformer for video generation. Latte first extracts spatio-temporal tokens from input videos and then adopts a series of Transformer blocks to model video distribution in the latent space. In order to model a substantial number of tokens extracted from videos, four efficient variants are introduced from the perspective of decomposing the spatial and temporal dimensions of input videos. To improve the quality of generated videos, we determine the best practices of Latte through rigorous experimental analysis, including video clip patch embedding, model variants, timestep-class information injection, temporal positional embedding, and learning strategies. Our comprehensive evaluation demonstrates that Latte achieves state-of-the-art performance across four standard video generation datasets, i.e., FaceForensics, SkyTimelapse, UCF101, and Taichi-HD. In addition, we extend Latte to the text-to-video generation (T2V) task, where Latte achieves results that are competitive with recent T2V models. We strongly believe that Latte provides valuable insights for future research on incorporating Transformers into diffusion models for video generation. Project page: &lt;a href=&#34;https://maxin-cn.github.io/latte_project&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://maxin-cn.github.io/latte_project&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;inference&#34;&gt;Inference&lt;/h2&gt;
&lt;p&gt;Latte-1 is now integrated into diffusers.&lt;/p&gt;
&lt;p&gt;You can easily run Latte using the following code. We also support inference with 4/8-bit quantization, which can reduce GPU memory from 17 GB to 9 GB.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;
&lt;pre class=&#34;chroma&#34;&gt;
&lt;code&gt;
#Please update the version of diffusers at leaset to 0.30.0
from diffusers import LattePipeline
from diffusers.models import AutoencoderKLTemporalDecoder
from torchvision.utils import save_image
import torch
import imageio
&lt;p&gt;torch.manual_seed(0)&lt;/p&gt;
&lt;p&gt;device = &amp;ldquo;cuda&amp;rdquo; if torch.cuda.is_available() else &amp;ldquo;cpu&amp;rdquo;
video_length = 16 # 1 (text-to-image) or 16 (text-to-video)
pipe = LattePipeline.from_pretrained(&amp;ldquo;maxin-cn/Latte-1&amp;rdquo;, torch_dtype=torch.float16).to(device)&lt;/p&gt;
&lt;p&gt;#Using temporal decoder of VAE
vae = AutoencoderKLTemporalDecoder.from_pretrained(&amp;ldquo;maxin-cn/Latte-1&amp;rdquo;, subfolder=&amp;ldquo;vae_temporal_decoder&amp;rdquo;, torch_dtype=torch.float16).to(device)
pipe.vae = vae&lt;/p&gt;
&lt;p&gt;prompt = &amp;ldquo;a cat wearing sunglasses and working as a lifeguard at pool.&amp;rdquo;
videos = pipe(prompt, video_length=video_length, output_type=&amp;lsquo;pt&amp;rsquo;).frames.cpu()
&lt;/code&gt;
&lt;/pre&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>üéâ one paper Cinemo was accpeted by IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
      <link>https://maxin-cn.github.io/post/cinemo-cvpr/</link>
      <pubDate>Thu, 27 Feb 2025 00:00:00 +0000</pubDate>
      <guid>https://maxin-cn.github.io/post/cinemo-cvpr/</guid>
      <description>&lt;p&gt;one paper &lt;a href=&#34;../../publication/conference-paper/cvpr_ma_2025/&#34;&gt;Cinemo&lt;/a&gt; was accpeted by IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)&lt;/p&gt;
&lt;h2 id=&#34;abstract&#34;&gt;Abstract&lt;/h2&gt;
&lt;p&gt;Diffusion models have achieved significant progress in the task of image animation due to their powerful generative capabilities. However, preserving appearance consistency to the static input image, and avoiding abrupt motion change in the generated animation, remains challenging.
In this paper, we introduce Cinemo, a novel image animation approach that aims at achieving better appearance consistency and motion smoothness. The core of Cinemo is to focus on learning the distribution of motion residuals, rather than directly predicting frames as in existing diffusion models.
During the inference, we further mitigate the sudden motion changes in the generated video by introducing a novel DCT-based noise refinement strategy.
To counteract the over-smoothing of motion, we introduce a dynamics degree control design for better control of the magnitude of motion.
Altogether, these strategies enable Cinemo to produce highly consistent, smooth, and motion-controllable results. Extensive experiments compared with several state-of-the-art methods demonstrate the effectiveness and superiority of our proposed approach.
In the end, we also demonstrate how our model can be applied for motion transfer or video editing of any given video. The project page is available at &lt;a href=&#34;https://maxin-cn.github.io/cinemo_project/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://maxin-cn.github.io/cinemo_project/&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;setup&#34;&gt;Setup&lt;/h2&gt;
&lt;p&gt;Download and set up the repo:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;
&lt;pre class=&#34;chroma&#34;&gt;
&lt;code&gt;
git clone https://github.com/maxin-cn/Cinemo
cd Cinemo
conda env create -f environment.yml
conda activate cinemo
&lt;/code&gt;
&lt;/pre&gt;
&lt;/div&gt;
&lt;h2 id=&#34;animation&#34;&gt;Animation&lt;/h2&gt;
&lt;p&gt;You can sample from our pre-trained Cinemo models. Weights for our pre-trained Cinemo model can be found &lt;a href=&#34;https://huggingface.co/maxin-cn/Cinemo/tree/main&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;. The script has various arguments for adjusting sampling steps, changing the classifier-free guidance scale, etc:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;
&lt;pre class=&#34;chroma&#34;&gt;
&lt;code&gt;
bash pipelines/animation.sh
&lt;/code&gt;
&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;Related model weights will be downloaded automatically, and the following results can be found &lt;a href=&#34;https://github.com/maxin-cn/Cinemo/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>üéâ one paper LaVie was accpeted by International Journal of Computer Vision (IJCV)</title>
      <link>https://maxin-cn.github.io/post/lavie-ijcv/</link>
      <pubDate>Mon, 28 Oct 2024 00:00:00 +0000</pubDate>
      <guid>https://maxin-cn.github.io/post/lavie-ijcv/</guid>
      <description>&lt;p&gt;one paper &lt;a href=&#34;../../publication/journal-article/ijcv_lavie_2024/&#34;&gt;LaVie&lt;/a&gt; was accpeted by International Journal of Computer Vision (IJCV)&lt;/p&gt;
&lt;h2 id=&#34;abstract&#34;&gt;Abstract&lt;/h2&gt;
&lt;p&gt;This work aims to learn a high-quality text-to-video (T2V) generative model by leveraging a pre-trained text-to-image (T2I) model as a basis. It is a highly desirable yet challenging task to simultaneously (a) accomplish the synthesis of visually realistic and temporally coherent videos while (b) preserving the strong creative generation nature of the pre-trained T2I model. To this end, we propose LaVie, an integrated video generation framework that operates on cascaded video latent diffusion models, comprising a base T2V model, a temporal interpolation model, and a video super-resolution model. Our key insights are two-fold: (1) We reveal that the incorporation of simple temporal self-attentions, coupled with rotary positional encoding, adequately captures the temporal correlations inherent in video data. (2) Additionally, we validate that the process of joint image-video fine-tuning plays a pivotal role in producing high-quality and creative outcomes. To enhance the performance of LaVie, we contribute a comprehensive and diverse video dataset named Vimeo25M, consisting of 25 million text-video pairs that prioritize quality, diversity, and aesthetic appeal. Extensive experiments demonstrate that LaVie achieves state-of-the-art performance both quantitatively and qualitatively. Furthermore, we showcase the versatility of pre-trained LaVie models in various long video generation and personalized video synthesis applications. Project page: &lt;a href=&#34;https://github.com/Vchitect/LaVie/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/Vchitect/LaVie/&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>üéâ one paper LEO was accpeted by International Journal of Computer Vision (IJCV)</title>
      <link>https://maxin-cn.github.io/post/leo-ijcv/</link>
      <pubDate>Tue, 20 Aug 2024 00:00:00 +0000</pubDate>
      <guid>https://maxin-cn.github.io/post/leo-ijcv/</guid>
      <description>&lt;p&gt;one paper &lt;a href=&#34;../../publication/journal-article/ijcv_leo_2024/&#34;&gt;LEO&lt;/a&gt; was accpeted by International Journal of Computer Vision (IJCV).&lt;/p&gt;
&lt;h2 id=&#34;abstract&#34;&gt;Abstract&lt;/h2&gt;
&lt;p&gt;Spatio-temporal coherency is a major challenge in synthesizing high quality videos, particularly in synthesizing human videos that contain rich global and local deformations. To resolve this challenge, previous approaches have resorted to different features in the generation process aimed at representing appearance and motion. However, in the absence of strict mechanisms to guarantee such disentanglement, a separation of motion from appearance has remained challenging, resulting in spatial distortions and temporal jittering that break the spatio-temporal coherency. Motivated by this, we here propose LEO, a novel framework for human video synthesis, placing emphasis on spatio-temporal coherency. Our key idea is to represent motion as a sequence of flow maps in the generation process, which inherently isolate motion from appearance. We implement this idea via a flow-based image animator and a Latent Motion Diffusion Model (LMDM). The former bridges a space of motion codes with the space of flow maps, and synthesizes video frames in a warp-and-inpaint manner. LMDM learns to capture motion prior in the training data by synthesizing sequences of motion codes. Extensive quantitative and qualitative analysis suggests that LEO significantly improves coherent synthesis of human videos over previous methods on the datasets TaichiHD, FaceForensics and CelebV-HQ. In addition, the effective disentanglement of appearance and motion in LEO allows for two additional tasks, namely infinite-length human video synthesis, as well as content-preserving video editing.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>üë©üèº‚Äçüè´ Invited talk on ‚ÄúApplication and expansion of DiT architecture in video generation models‚Äù at Intelligent things</title>
      <link>https://maxin-cn.github.io/post/invited-talk-latte/</link>
      <pubDate>Fri, 14 Jun 2024 00:00:00 +0000</pubDate>
      <guid>https://maxin-cn.github.io/post/invited-talk-latte/</guid>
      <description>&lt;p&gt;I have been invited to give a &lt;a href=&#34;../../event&#34;&gt;talk&lt;/a&gt; on &amp;ldquo;Application and Expansion of the DiT Architecture in Video Generation Models&amp;rdquo; at &lt;a href=&#34;https://course.zhidx.com/topic/detail/YmE3M2QxN2I4YmE0M2IwMGQ0MjM=&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Intelligent Things&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;abstract&#34;&gt;Abstract&lt;/h2&gt;
&lt;p&gt;In this talk, I will share the current state and recent advances in video generation research. I will then introduce Latte, a Transformer-based video diffusion model. Following that, I will present some visual comparisons of generated videos. Finally, I will conclude with a discussion on potential future directions and task extensions in text-to-video generation.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>‚úÖ Release the codes of Cinemo</title>
      <link>https://maxin-cn.github.io/post/cinemo-code/</link>
      <pubDate>Sun, 02 Jun 2024 00:00:00 +0000</pubDate>
      <guid>https://maxin-cn.github.io/post/cinemo-code/</guid>
      <description>&lt;p&gt;We release the &lt;a href=&#34;https://github.com/maxin-cn/Cinemo&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;codes&lt;/a&gt; of &lt;a href=&#34;https://arxiv.org/abs/2407.15642&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Cinemo&lt;/a&gt; on Github.&lt;/p&gt;
&lt;h2 id=&#34;setup&#34;&gt;Setup&lt;/h2&gt;
&lt;p&gt;Download and set up the repo:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;
&lt;pre class=&#34;chroma&#34;&gt;
&lt;code&gt;
git clone https://github.com/maxin-cn/Cinemo
cd Cinemo
conda env create -f environment.yml
conda activate cinemo
&lt;/code&gt;
&lt;/pre&gt;
&lt;/div&gt;
&lt;h2 id=&#34;animation&#34;&gt;Animation&lt;/h2&gt;
&lt;p&gt;You can sample from our pre-trained Cinemo models. Weights for our pre-trained Cinemo model can be found &lt;a href=&#34;https://huggingface.co/maxin-cn/Cinemo/tree/main&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;. The script has various arguments for adjusting sampling steps, changing the classifier-free guidance scale, etc:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;
&lt;pre class=&#34;chroma&#34;&gt;
&lt;code&gt;
bash pipelines/animation.sh
&lt;/code&gt;
&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;Related model weights will be downloaded automatically, and the following results can be found &lt;a href=&#34;https://github.com/maxin-cn/Cinemo/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>‚úÖ Release the checkpoints of Latte-1</title>
      <link>https://maxin-cn.github.io/post/latte-code/</link>
      <pubDate>Thu, 23 May 2024 00:00:00 +0000</pubDate>
      <guid>https://maxin-cn.github.io/post/latte-code/</guid>
      <description>&lt;!-- 

&lt;details class=&#34;toc-inpage d-print-none  &#34; open&gt;
  &lt;summary class=&#34;font-weight-bold&#34;&gt;Table of Contents&lt;/summary&gt;
  &lt;nav id=&#34;TableOfContents&#34;&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;a href=&#34;#inference&#34;&gt;Inference&lt;/a&gt;&lt;/li&gt;
  &lt;/ul&gt;
&lt;/nav&gt;
&lt;/details&gt;
 --&gt;
&lt;p&gt;We release the &lt;a href=&#34;https://huggingface.co/maxin-cn/Latte-1&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;chckpoints&lt;/a&gt; of &lt;a href=&#34;https://github.com/Vchitect/Latte&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Latte-1&lt;/a&gt; on Hugging Face.&lt;/p&gt;
&lt;h2 id=&#34;inference&#34;&gt;Inference&lt;/h2&gt;
&lt;p&gt;Latte-1 is now integrated into diffusers.&lt;/p&gt;
&lt;p&gt;You can easily run Latte using the following code. We also support inference with 4/8-bit quantization, which can reduce GPU memory from 17 GB to 9 GB.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;
&lt;pre class=&#34;chroma&#34;&gt;
&lt;code&gt;
#Please update the version of diffusers at leaset to 0.30.0
from diffusers import LattePipeline
from diffusers.models import AutoencoderKLTemporalDecoder
from torchvision.utils import save_image
import torch
import imageio
&lt;p&gt;torch.manual_seed(0)&lt;/p&gt;
&lt;p&gt;device = &amp;ldquo;cuda&amp;rdquo; if torch.cuda.is_available() else &amp;ldquo;cpu&amp;rdquo;
video_length = 16 # 1 (text-to-image) or 16 (text-to-video)
pipe = LattePipeline.from_pretrained(&amp;ldquo;maxin-cn/Latte-1&amp;rdquo;, torch_dtype=torch.float16).to(device)&lt;/p&gt;
&lt;p&gt;#Using temporal decoder of VAE
vae = AutoencoderKLTemporalDecoder.from_pretrained(&amp;ldquo;maxin-cn/Latte-1&amp;rdquo;, subfolder=&amp;ldquo;vae_temporal_decoder&amp;rdquo;, torch_dtype=torch.float16).to(device)
pipe.vae = vae&lt;/p&gt;
&lt;p&gt;prompt = &amp;ldquo;a cat wearing sunglasses and working as a lifeguard at pool.&amp;rdquo;
videos = pipe(prompt, video_length=video_length, output_type=&amp;lsquo;pt&amp;rsquo;).frames.cpu()
&lt;/code&gt;
&lt;/pre&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
